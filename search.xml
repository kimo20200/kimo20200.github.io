<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Detection of proliferation indices from microscopic image for tumour progression analysis</title>
      <link href="/2020/12/06/Research_UROS/"/>
      <url>/2020/12/06/Research_UROS/</url>
      
        <content type="html"><![CDATA[<h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>Glioblastoma (GBM) is the most aggressive and common type of brain tumour in adults. The main challenges patients face are the low survival rate, extraordinarily high tumour heterogeneity, and lack of specific treatments of GBM [1]. In classic clinical practices, histopathology images are manually analysed by medical experts or pathologists for diagnosis of disease stage [2]. However, manual analysis can cause some problems:</p><ul><li><p>There is no standard assessment for the diagnosis. Experiences and subjectivity of medical professionals or pathologist can impact evaluation criteria significantly [3]. </p></li><li><p>It is a protracted and monotonous task that pathologists visually navigate and review glass slides or whole slide images (WSI) to detect and analyse malformations in daily work [4]. After evaluating the brain graphics, if tumour existence is disbelieved, the patient’s brain biopsy will be activated. Unlike magnetic resonance (MR), biopsy has an invasive process, and sometimes, it may even consume a month to determine an answer [5].</p></li></ul><p>Therefore, the result of the analysis is not quantitative. For these cases, an efficient and quick method of analysing images is urgently needed.</p><h4 id="Objectives"><a href="#Objectives" class="headerlink" title="Objectives"></a>Objectives</h4><p>The main goals of this research are:</p><ol><li>To establish a computer-based system that can detect and classify different types of tumour cells.</li><li>Finding out the percentage of positive cells to all cells shown in the image.</li><li>Reducing the costs in viewing time, examination and interference from human factors, thereby assisting pathologists to improve the accuracy in clinical diagnosis.</li></ol><h4 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h4><h5 id="Manual-extraction-of-training-and-validation-dataset"><a href="#Manual-extraction-of-training-and-validation-dataset" class="headerlink" title="Manual extraction of training and validation dataset"></a>Manual extraction of training and validation dataset</h5><p>Three of 22 whole slide images were randomly selected as testing samples for this research. The rest of images are used to train our network. All cells in these images were extracted and divided into five classes: single positive cells (SP), single negative cells (SN), connected positive cells (CP) and connected negative cells (CN). Also, the background information (BG) was collected as negative samples for the training dataset. Some examples of the training samples are presented in figure 1.<br><img src="https://cdn.jsdelivr.net/gh/xq14183903/xq14183903.github.io@latest/images/research/uros/1.png" alt="" loading="lazy"><em>Figure 1-cell classification overview</em></p><h5 id="Deep-Residual-Network-ResNet-classification"><a href="#Deep-Residual-Network-ResNet-classification" class="headerlink" title="Deep Residual Network (ResNet) classification"></a>Deep Residual Network (ResNet) classification</h5><p>In order to classify different classes of cells based on different sizes of patches from the detector, a popular classifier called ResNet50 [6], was chosen to train on the extracted dataset with stochastic gradient descent (SGD) optimizer, momentum of 0.9, learning rate of 0.002, and cross entropy as loss function.<br><img src="https://cdn.jsdelivr.net/gh/xq14183903/xq14183903.github.io@latest/images/research/uros/2.png" alt="" loading="lazy"><em>Figure 2, the architecture layout of ResNet50</em></p><h5 id="Cell-detection-and-segmentation"><a href="#Cell-detection-and-segmentation" class="headerlink" title="Cell detection and segmentation"></a>Cell detection and segmentation</h5><p>While testing images, cells are detected and identifies by using sliding window with softer non-maximum suppression (NMS) [7]. In addition, the size of bounding boxes has been configurated in advance. All of connected cells identified by trained classifier are then been split into a single cell based on the watershed algorithm. Finally, the detected cells were counted based on their predicted label. </p><h5 id="Evaluation-criteria"><a href="#Evaluation-criteria" class="headerlink" title="Evaluation criteria"></a>Evaluation criteria</h5><p>To assess the performance of our classifier, we used precision, recall and F-score as performance measurement matrices derived from true positive (TP), true negatives (TN), false negatives (FN) and false positive (FP) values. Also, we compare the predicted proliferation index that obtained from detection to ground truth and show the difference as error rate.<br>Precision, recall, F-score and error rate are defined as follows:</p><p>$$ Precision=\frac{TP}{TP+FP} $$<br>$$ Recall=\frac{TP}{TP+FN} $$<br>$$ F=\frac{2(Recall*Precision)}{Recall+Precision} $$<br>$$ Error rate=\frac{DL-Munually}{Munually} $$</p><h4 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h4><p>To make full use of limited datasets and prevent the over-fit, K-fold cross validation was used for training the model. The cells that extracted from original images were split into five subsets before training. Every subset contains 20% of the total data. We performed 50 epochs with 5-fold cross validation to train the model.<br><img src="https://cdn.jsdelivr.net/gh/xq14183903/xq14183903.github.io@latest/images/research/uros/3.png" alt="" loading="lazy"><em>Figure 3 – loss and accuracy vs. the number of epochs</em><br>Figure 3 shows the accuracy and loss values in training and validating phase for each 5-fold cross validation. The highest validated accuracy of model reached 93.3%, which was selected as the classifier to recognise cells.<br>For evaluating performance of the model, we prepared three original-size images (1392×1040 for the size) that are unseen for the model. The result of the assessment has been demonstrated in table 1.<br><img src="https://cdn.jsdelivr.net/gh/xq14183903/xq14183903.github.io@latest/images/research/uros/4.png" alt="" loading="lazy"><em>Table 1 – evaluation measure on three test images</em><br>Furthermore, a comparison of calculating proliferation manually and based on our trained model has been indicated in the Table 2.<br><img src="https://cdn.jsdelivr.net/gh/xq14183903/xq14183903.github.io@latest/images/research/uros/5.png" alt="" loading="lazy"><em>Table 2 – proliferation score concluded by different measures and comparison</em><br>Detection time is one of important factors of object recognition. Generally, the algorithm is supposed to keep a high accuracy of detection and complete the task as soon as possible. In our experiment, it took an average of 13 minutes for identifying each image.<br>Moreover, the cell detection and segmentation algorithm were implemented using Python and OpenCV tools on a machine with AMD 3500X processor, 16GB RAM, NVIDIA RTX2060 GPU. </p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>In this study, we propose an approach combined with deep learning and image segmentation to detect, categorize and count cells in the GBM histological images. The model implemented provides high accuracy and performance according to the result of quantitative evaluation that includes precision, recall and F-score. Compared with manual annotations, the proposed model shows an acceptable error rate of cells recognition.</p><p>However, this model still has some shortages. It is time-consuming to detect cells based on sliding window algorithm. Although the time cost can be reduced by increasing step sizes of sliding window, the recall rate of detection will be decreased simultaneously. Also, configured bounding boxes constrict the size of cells that can be detected. If the size of cells exceeds these bounding boxes, the detector may fail. Further studies should concentrate more on optimization of object detection and segmentation. Some new methods, such as CBNet and YOLO v4 should be considered in future research to improve the performance.</p><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><blockquote><ol><li>Lee, E., Yong, R.L., Paddison, P. and Zhu, J., 2018, December. Comparison of glioblastoma (GBM) molecular classification methods. In Seminars in cancer biology (Vol. 53, pp. 201-211). Academic Press.</li><li>Chen, R., Smith-Cohn, M., Cohen, A.L. and Colman, H., 2017. Glioma subclassifications and their clinical significance. Neurotherapeutics, 14(2), pp.284-297.</li><li>Yonekura, A., Kawanaka, H., Prasath, V.S., Aronow, B.J. and Takase, H., 2017, September. Glioblastoma multiforme tissue histopathology images-based disease stage classification with deep CNN. In 2017 6th International Conference on Informatics, Electronics and Vision &amp; 2017 7th International Symposium in Computational Medical and Health Technology (ICIEV-ISCMHT) (pp. 1-5). IEEE.</li><li>Sharma, H., Zerbe, N., Klempert, I., Hellwich, O. and Hufnagl, P., 2017. Deep convolutional neural networks for automatic classification of gastric carcinoma using whole slide images in digital histopathology. Computerized Medical Imaging and Graphics, 61, pp.2-13.</li><li>Anaraki, A.K., Ayati, M. and Kazemi, F., 2019. Magnetic resonance imaging-based brain tumor grades classification and grading via convolutional neural networks and genetic algorithms. Biocybernetics and Biomedical Engineering, 39(1), pp.63-74.</li><li>He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).</li><li>He, Y., Zhang, X., Savvides, M. and Kitani, K., 2018. Softer-nms: Rethinking bounding box regression for accurate object detection. arXiv preprint arXiv:1809.08545, 2.</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Medical Image Classification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络模型[Pytorch]</title>
      <link href="/2020/08/19/DL_P9/"/>
      <url>/2020/08/19/DL_P9/</url>
      
        <content type="html"><![CDATA[<h4 id="网络模型库"><a href="#网络模型库" class="headerlink" title="网络模型库"></a>网络模型库</h4><p>对于深度学习，torchvision.models库提供了众多经典的网络结构与预训练模型，例如VGG、ResNet和Inception等，利用这些模型可以快速搭建物体检测网络，不需要逐层手动实现。<br>例: Resnet_50</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from torchvision import models</span><br><span class="line">resnet &#x3D; models.resnet50()</span><br><span class="line"># 查看网络框架</span><br><span class="line">print(len(resnet.layer1))</span><br><span class="line"># 可以通过出现的顺序直接索引每一层</span><br><span class="line">print(resnet.layer1[0])</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">3</span><br><span class="line">Bottleneck(</span><br><span class="line">  (conv1): Conv2d(64, 64, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">  (bn1): BatchNorm2d(64, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">  (conv2): Conv2d(64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">  (bn2): BatchNorm2d(64, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">  (conv3): Conv2d(64, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">  (bn3): BatchNorm2d(256, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">  (relu): ReLU(inplace&#x3D;True)</span><br><span class="line">  (downsample): Sequential(</span><br><span class="line">    (0): Conv2d(64, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1), bias&#x3D;False)</span><br><span class="line">    (1): BatchNorm2d(256, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="加载预训练模型"><a href="#加载预训练模型" class="headerlink" title="加载预训练模型"></a>加载预训练模型</h4><ol><li>torchvision.models中自带的预训练模型，只需要在使用时赋予pretrained参数为True即可。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from torchvision import models</span><br><span class="line">resnet &#x3D; models.resnet50(pretrained &#x3D; True)</span><br></pre></td></tr></table></figure></li><li>如果想要使用自己的本地预训练模型，或者之前训练过的模型，则可以通过model.load_state_dict()函数操作<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torchvision import models</span><br><span class="line">resnet &#x3D; models.resnet50(pretrained &#x3D; False)</span><br><span class="line">state_dict &#x3D; torch.load(&#39;Resnet_four_classification&#x2F;model1.pth&#39;)</span><br><span class="line">resnet.load_state_dict(state_dict,False)</span><br></pre></td></tr></table></figure></li></ol><ul><li>通常来讲，对于不同的检测任务，卷积网络的前两三层的作用是非常类似的，都是提取图像的边缘信息等，因此为了保证模型训练中能够更加稳定，一般会固定预训练网络的前两三个卷积层而不进行参数的学习<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">frozen_layers &#x3D; [resnet.conv1,resnet.layer2,resnet.layer3]</span><br><span class="line">for layer in frozen_layers:</span><br><span class="line">    for name, value in layer.named_parameters():</span><br><span class="line">        value.requires_grad &#x3D; False</span><br></pre></td></tr></table></figure></li></ul><h4 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h4><ol><li>直接保存整个模型<br>例：<code>torch.save(model_ft, &#39;model.pth&#39;)</code></li><li>选择性保存<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torch.save(&#123;</span><br><span class="line">            &#39;epoch&#39;: epoch,</span><br><span class="line">            &#39;model_state_dict&#39;: model.state_dict(),</span><br><span class="line">            &#39;optimizer_state_dict&#39;: optimizer.state_dict(),</span><br><span class="line">            &#39;loss&#39;: loss,</span><br><span class="line">            ...</span><br><span class="line">            &#125;, PATH)</span><br></pre></td></tr></table></figure></li></ol><h4 id="数据处理，加载，变换与增强"><a href="#数据处理，加载，变换与增强" class="headerlink" title="数据处理，加载，变换与增强"></a>数据处理，加载，变换与增强</h4><p>详细信息请见这两篇文章：</p><blockquote><p><a href="https://xq14183903.github.io/2020/07/25/DL_P3/" target="_blank" rel="noopener">数据集图片变换与增强[transform][augmentation]</a><br><a href="https://xq14183903.github.io/2020/07/24/DL_P2/" target="_blank" rel="noopener">训练数据准备与导入[自定义数据集][分类问题]</a></p></blockquote><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><blockquote><p>深度学习之PyTorch物体检测实战 - 董洪义</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensor[Pytorch][3]</title>
      <link href="/2020/08/17/DL_P8/"/>
      <url>/2020/08/17/DL_P8/</url>
      
        <content type="html"><![CDATA[<h4 id="Tensor的自动求导"><a href="#Tensor的自动求导" class="headerlink" title="Tensor的自动求导"></a>Tensor的自动求导</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.randn(2,3,requires_grad &#x3D; True)</span><br><span class="line">b &#x3D; torch.randn(2,3)</span><br><span class="line">print(b.requires_grad)</span><br><span class="line"># 只要被计算中的任意一个变量需要求导，那么返回的变量也需要求导</span><br><span class="line">c &#x3D; a + b</span><br><span class="line"># 分离变量，返回的变量不需要求导</span><br><span class="line">d &#x3D; a.detach()</span><br><span class="line">b.requires_grad_()</span><br><span class="line"># 显示变量c经过了什么操作</span><br><span class="line">print(c.grad_fn)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">print(b.requires_grad)</span><br><span class="line">print(c.requires_grad)</span><br><span class="line">print(d.requires_grad)</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">False</span><br><span class="line">&lt;AddBackward0 object at 0x000002672E8FDC88&gt;</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure><p><strong>grad：</strong>该Tensor对应的梯度，类型为Tensor，并与Tensor同维度。<br><strong>grad_fn：</strong>指向function对象，即该Tensor经过了什么样的操作，用作反向传播的梯度计算，如果该Tensor由用户自己创建，则该grad_fn为None。</p><ul><li>require_grad参数表示是否需要对该Tensor进行求导，默认为False；设置为True则需要求导，并且依赖于该Tensor的之后的所有节点都需要求导。</li><li>默认的Tensor是不需要求导的</li></ul><h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><p>计算图是PyTorch对于神经网络的具体实现形式，包括每一个数据Tensor及Tensor之间的函数function。<br>Autograd的基本原理是随着每一步Tensor的计算操作，逐渐生成计算图，并将操作的function记录在Tensor的grad_fn中。在前向计算完后，只需对根节点进行backward函数操作，即可从当前根节点自动进行反向传播与梯度计算，从而得到每一个叶子节点的梯度，梯度计算遵循链式求导法则。<br>例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"># 自己生成的，因此都为叶节点</span><br><span class="line">x &#x3D; torch.randn(1)</span><br><span class="line">w &#x3D; torch.ones(1, requires_grad&#x3D;True)</span><br><span class="line">b &#x3D; torch.ones(1, requires_grad&#x3D;True)</span><br><span class="line">print(b)</span><br><span class="line">print(x.is_leaf)</span><br><span class="line">print(w.is_leaf)</span><br><span class="line">print(b.is_leaf)</span><br><span class="line"># 进行前向计算，由计算生成的变量都不是叶节点</span><br><span class="line">y &#x3D; x * w</span><br><span class="line">z &#x3D; y &#x3D; b</span><br><span class="line">print(y.is_leaf)</span><br><span class="line">print(z.is_leaf)</span><br><span class="line">print(w.grad)</span><br><span class="line">print(b.grad)</span><br><span class="line"># 对根节点调用backward()函数，进行梯度反传</span><br><span class="line">z.backward(retain_graph &#x3D; True)</span><br><span class="line">print(w.grad)</span><br><span class="line">print(b.grad)</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">None</span><br><span class="line">None</span><br><span class="line">None</span><br><span class="line">tensor([1.])</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure><ul><li>backward()函数还有一个需要传入的参数grad_variabels，其代表了根节点的导数，也可以看做根节点各部分的权重系数。因为PyTorch不允许Tensor对Tensor求导，求导时都是标量对于Tensor进行求导，因此，如果根节点是向量，则应配以对应大小的权重，并求和得到标量，再反传。如果根节点的值是标量，则该参数可以省略，默认为1。</li><li>当有多个输出需要同时进行梯度反传时，需要将retain_graph设置为True，从而保证在计算多个输出的梯度时互不影响。</li></ul><h4 id="nn-Module类"><a href="#nn-Module类" class="headerlink" title="nn.Module类"></a>nn.Module类</h4><p>nn.Module是PyTorch提供的神经网络类，并在类中实现了网络各层的定义及前向计算与反向传播机制。在实际使用时，如果想要实现某个神经网络，只需继承nn.Module，在初始化中定义模型结构与参数，在函数forward()中编写网络前向过程即可。<br>例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line"># 首先建立一个全连接的子module，继承nn.Module</span><br><span class="line">class Linear(nn.Module):</span><br><span class="line">   def __init__(self, in_dim, out_dim):</span><br><span class="line">       super(Linear, self).__init__()   # 调用nn.Module的构造函数</span><br><span class="line">       # 使用nn.Parameter来构造需要学习的参数</span><br><span class="line">       self.w &#x3D; nn.Parameter(torch.randn(in_dim, out_dim))</span><br><span class="line">       self.b &#x3D; nn.Parameter(torch.randn(out_dim))</span><br><span class="line">   # 在forward中实现前向传播过程</span><br><span class="line">   def forward(self, x):</span><br><span class="line">        x &#x3D; x.matmul(self.w)            # 使用Tensor.matmul实现矩阵相乘</span><br><span class="line">        y &#x3D; x + self.b.expand_as(x)     # 使用Tensor.expand_as()来保证矩阵形状一致</span><br><span class="line">        return y</span><br><span class="line">class Perception(nn.Module):</span><br><span class="line">   def __init__(self, in_dim, hid_dim, out_dim):</span><br><span class="line">       super(Perception, self).__init__()</span><br><span class="line">       self.layer1 &#x3D; Linear(in_dim, hid_dim)</span><br><span class="line">       self.relu &#x3D; nn.ReLU(inplace&#x3D;True)</span><br><span class="line">       self.layer2 &#x3D; Linear(hid_dim, out_dim)</span><br><span class="line">       self.fc &#x3D; nn.Linear(3, 3)</span><br><span class="line">   def forward(self, x):</span><br><span class="line">       x &#x3D; self.layer1(x)</span><br><span class="line">       y &#x3D; torch.relu(x)                # 使用torch中的relu作为激活函数</span><br><span class="line">       y &#x3D; self.layer2(y)   </span><br><span class="line">       y &#x3D; torch.softmax(y,dim &#x3D; 1)     # 使用torch中的softmax作为激活函数来进行多分类</span><br><span class="line">       return y</span><br><span class="line"># 实例化一个网络，并赋值全连接中的维数，最终输出三维代表了三分类</span><br><span class="line">pcp &#x3D; Perception(3, 3, 3)</span><br><span class="line">print(pcp)</span><br><span class="line"># named_parameters()可以返回学习参数的迭代器，分别为参数名与参数值</span><br><span class="line">for name, parameter in pcp.named_parameters():</span><br><span class="line">    print(name, parameter)</span><br><span class="line">data &#x3D; torch.randn(4,3)</span><br><span class="line">print(pcp(data))</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line">感知器结构：</span><br><span class="line">Perception(</span><br><span class="line">  (layer1): Linear()</span><br><span class="line">  (relu): ReLU(inplace&#x3D;True)</span><br><span class="line">  (layer2): Linear()</span><br><span class="line">)</span><br><span class="line">各个层的参数：</span><br><span class="line">layer1.w Parameter containing:</span><br><span class="line">tensor([[-0.1038, -0.8919,  0.5721],</span><br><span class="line">        [ 1.0064, -0.2040,  1.1663],</span><br><span class="line">        [-1.6619,  0.6891,  0.9040]], requires_grad&#x3D;True)</span><br><span class="line">layer1.b Parameter containing:</span><br><span class="line">tensor([-0.8985, -0.9285,  0.4434], requires_grad&#x3D;True)</span><br><span class="line">layer2.w Parameter containing:</span><br><span class="line">tensor([[-1.2363,  0.2189,  0.2963],</span><br><span class="line">        [-0.1473,  1.8698,  0.7392],</span><br><span class="line">        [-0.7365,  1.0238, -0.2207]], requires_grad&#x3D;True)</span><br><span class="line">layer2.b Parameter containing:</span><br><span class="line">tensor([ 2.0176, -0.2706,  1.5695], requires_grad&#x3D;True)</span><br><span class="line">测试结果：</span><br><span class="line">tensor([[0.5746, 0.0583, 0.3671],</span><br><span class="line">        [0.3566, 0.2445, 0.3988],</span><br><span class="line">        [0.3658, 0.2336, 0.4006],</span><br><span class="line">        [0.4451, 0.1455, 0.4094]], grad_fn&#x3D;&lt;SoftmaxBackward&gt;)</span><br></pre></td></tr></table></figure><ul><li>nn.Parameter()函数定义了全连接中的ω和b，这是一种特殊的Tensor的构造方法，默认需要求导，即requires_grad为True。</li><li>在PyTorch中，还有一个库为nn.functional，同样也提供了很多网络层与函数功能，但与nn.Module不同的是，利用nn.functional定义的网络层不可自动学习参数，还需要使用nn.Parameter封装。nn.functional的设计初衷是对于一些不需要学习参数的层，如激活层、BN（Batch Normalization）层，可以使用nn.functional，这样这些层就不需要在nn.Module中定义了。</li><li>当模型中只是简单的前馈网络时，即上一层的输出直接作为下一层的输入，这时可以采用nn.Sequential()模块来快速搭建模型，而不必手动在forward()函数中一层一层地前向传播。因此，如果想快速搭建模型而不考虑中间过程的话，推荐使用nn.Sequential()模块。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line">class Perception(nn.Module):</span><br><span class="line">   def __init__(self, in_dim, hid_dim, out_dim):</span><br><span class="line">       super(Perception, self).__init__()</span><br><span class="line">       self.layer &#x3D; nn.Sequential(</span><br><span class="line">             nn.Linear(in_dim, hid_dim),</span><br><span class="line">             nn.Sigmoid(),</span><br><span class="line">             nn.Linear(hid_dim, out_dim),</span><br><span class="line">             nn.Sigmoid()</span><br><span class="line">)</span><br><span class="line">   def forward(self, x):</span><br><span class="line">        y &#x3D; self.layer(x)</span><br><span class="line">        return y</span><br><span class="line">        pcp &#x3D; Perception(1000,10000 ,10)</span><br><span class="line">print(pcp)</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">Perception(</span><br><span class="line">  (layer): Sequential(</span><br><span class="line">    (0): Linear(in_features&#x3D;1000, out_features&#x3D;10000, bias&#x3D;True)</span><br><span class="line">    (1): Sigmoid()</span><br><span class="line">    (2): Linear(in_features&#x3D;10000, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">    (3): Sigmoid()</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li></ul><h4 id="损失函数的选择"><a href="#损失函数的选择" class="headerlink" title="损失函数的选择"></a>损失函数的选择</h4><p>PyTorch在torch.nn及torch.nn.functional中都提供了各种损失函数，通常来讲，由于损失函数不含有可学习的参数，因此这两者在功能上基本没有区别。</p><table><thead><tr><th>问题类型</th><th>损失函数</th></tr></thead><tbody><tr><td>二分类</td><td>nn.CrossEntropyLoss()</td></tr><tr><td>多类别分类</td><td>nn.CrossEntropyLoss()</td></tr><tr><td>多标签分类</td><td>nn.CrossEntropyLoss()</td></tr><tr><td>回归</td><td>MSE</td></tr><tr><td>向量回归</td><td>MSE</td></tr></tbody></table><p>例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"># 首先建立一个全连接的子module，继承nn.Module</span><br><span class="line">class Linear(nn.Module):</span><br><span class="line">   def __init__(self, in_dim, out_dim):</span><br><span class="line">       super(Linear, self).__init__()   # 调用nn.Module的构造函数</span><br><span class="line">       # 使用nn.Parameter来构造需要学习的参数</span><br><span class="line">       self.w &#x3D; nn.Parameter(torch.randn(in_dim, out_dim))</span><br><span class="line">       self.b &#x3D; nn.Parameter(torch.randn(out_dim))</span><br><span class="line">   # 在forward中实现前向传播过程</span><br><span class="line">   def forward(self, x):</span><br><span class="line">        x &#x3D; x.matmul(self.w)            # 使用Tensor.matmul实现矩阵相乘</span><br><span class="line">        y &#x3D; x + self.b.expand_as(x)     # 使用Tensor.expand_as()来保证矩阵形状一致</span><br><span class="line">        return y</span><br><span class="line">class Perception(nn.Module):</span><br><span class="line">   def __init__(self, in_dim, hid_dim, out_dim):</span><br><span class="line">       super(Perception, self).__init__()</span><br><span class="line">       self.layer1 &#x3D; Linear(in_dim, hid_dim)</span><br><span class="line">       self.relu &#x3D; nn.ReLU(inplace&#x3D;True)</span><br><span class="line">       self.layer2 &#x3D; Linear(hid_dim, out_dim)</span><br><span class="line">       self.fc &#x3D; nn.Linear(3, 3)</span><br><span class="line">   def forward(self, x):</span><br><span class="line">       x &#x3D; self.layer1(x)</span><br><span class="line">       y &#x3D; torch.relu(x)                # 使用torch中的relu作为激活函数</span><br><span class="line">       y &#x3D; self.layer2(y)   </span><br><span class="line">       y &#x3D; torch.softmax(y,dim &#x3D; 1)     # 使用torch中的softmax作为激活函数来进行多分类</span><br><span class="line">       return y</span><br><span class="line"># 实例化一个网络，并赋值全连接中的维数，最终输出三维代表了三分类</span><br><span class="line">pcp &#x3D; Perception(3, 3, 3)</span><br><span class="line">print(pcp)</span><br><span class="line"># named_parameters()可以返回学习参数的迭代器，分别为参数名与参数值</span><br><span class="line">for name, parameter in pcp.named_parameters():</span><br><span class="line">    print(name, parameter)</span><br><span class="line">data &#x3D; torch.randn(4,3)</span><br><span class="line">output &#x3D; pcp(data)</span><br><span class="line">print(output)</span><br><span class="line"># 设置标签，每个数为0、1或2三个类别</span><br><span class="line">label &#x3D; torch.Tensor([0, 1, 2, 0]).long()</span><br><span class="line"># 实例化nn中的交叉熵损失类</span><br><span class="line">criterion &#x3D; nn.CrossEntropyLoss()</span><br><span class="line">loss_functional &#x3D; F.cross_entropy(output, label)</span><br><span class="line">print(criterion(output, label), loss_functional)</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">Perception(</span><br><span class="line">  (layer1): Linear()</span><br><span class="line">  (relu): ReLU(inplace&#x3D;True)</span><br><span class="line">  (layer2): Linear()</span><br><span class="line">  (fc): Linear(in_features&#x3D;3, out_features&#x3D;3, bias&#x3D;True)</span><br><span class="line">)</span><br><span class="line">layer1.w Parameter containing:</span><br><span class="line">tensor([[-0.5057, -0.3412,  1.3228],</span><br><span class="line">        [ 0.2286,  0.7570,  1.0803],</span><br><span class="line">        [-0.1114, -0.8957,  0.1538]], requires_grad&#x3D;True)</span><br><span class="line">layer1.b Parameter containing:</span><br><span class="line">tensor([ 0.6814, -0.4924, -0.3002], requires_grad&#x3D;True)</span><br><span class="line">layer2.w Parameter containing:</span><br><span class="line">tensor([[ 2.3430,  1.0380,  1.0289],</span><br><span class="line">        [ 0.5373,  0.5528, -1.4240],</span><br><span class="line">        [-1.2860,  0.2747,  1.0854]], requires_grad&#x3D;True)</span><br><span class="line">layer2.b Parameter containing:</span><br><span class="line">tensor([-0.5483, -0.8220, -0.1983], requires_grad&#x3D;True)</span><br><span class="line">fc.weight Parameter containing:</span><br><span class="line">tensor([[-0.5237, -0.1501,  0.5000],</span><br><span class="line">        [ 0.2985,  0.4648, -0.1471],</span><br><span class="line">        [-0.4871, -0.3018,  0.2251]], requires_grad&#x3D;True)</span><br><span class="line">fc.bias Parameter containing:</span><br><span class="line">tensor([0.4509, 0.1335, 0.0446], requires_grad&#x3D;True)</span><br><span class="line">tensor([[0.0069, 0.1075, 0.8856],</span><br><span class="line">        [0.0141, 0.1204, 0.8655],</span><br><span class="line">        [0.4050, 0.2080, 0.3870],</span><br><span class="line">        [0.0056, 0.0956, 0.8988]], grad_fn&#x3D;&lt;SoftmaxBackward&gt;)</span><br><span class="line">两者求得的损失值相同</span><br><span class="line">tensor(1.3640, grad_fn&#x3D;&lt;NllLossBackward&gt;) tensor(1.3640, grad_fn&#x3D;&lt;NllLossBackward&gt;)</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure><h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><ol><li>SGD<br>例：<br><code>optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.002, momentum=0.9,weight_decay=1e-5)</code></li><li>Adam<br>例：<br><code>optimizer_ft = optim.Adam(model_ft.parameters(),lr=0.002,weight_decay=1e-4)</code></li><li>RMSprop<br>例：<br><code>optimizer_ft = torch.optim.RMSprop(model_ft.parameters(),lr=0.002,alpha=0.9)</code> </li></ol><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><blockquote><p>深度学习之PyTorch物体检测实战 - 董洪义<br>Subramanian, V., 2018. Deep Learning with PyTorch: A practical approach to building neural network models using PyTorch. Packt Publishing Ltd.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensor[Pytorch][2]</title>
      <link href="/2020/08/15/DL_P7/"/>
      <url>/2020/08/15/DL_P7/</url>
      
        <content type="html"><![CDATA[<h4 id="Tensor的索引与变形"><a href="#Tensor的索引与变形" class="headerlink" title="Tensor的索引与变形"></a>Tensor的索引与变形</h4><p>先行变量：<code>a = torch.Tensor([[0,1],[2,3]])</code></p><ol><li>根据下标进行索引（类似数组索引）<br>例：<code>a[0]</code>, <code>a[0,1]</code></li><li>设置条件对tensor内的元素进行判断，符合条件的置True，否则置False<br>例：<code>b = a &gt; 1</code></li><li>选择符合条件的元素并返回<br>例：<code>a[a &gt; 1]</code></li><li>选择非0元素的坐标，并返回<br><code>torch.nonzero()</code></li><li>满足condition的位置输出x，否则输出y<br><code>torch.where(condition, torch.full_like(input, x), y)</code></li><li>限制Tensor元素在[x, y]范围内，小于x的元素被设置成x，大于y的元素被设置成y，其余的不变<br><code>input.clamp(x, y)</code></li><li>view()、resize()和reshape()函数可以在不改变Tensor数据的前提下任意改变Tensor的形状，必须保证调整前后的元素总数相同，并且调整前后共享内存，三者的作用基本相同。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.Tensor([[0,1],[2,3]])</span><br><span class="line">print(a.view(1,4)) </span><br><span class="line">print(a.resize(2,2))</span><br><span class="line">print(a.reshape(4,1))</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">tensor([[0., 1., 2., 3.]])</span><br><span class="line">tensor([[0., 1.],</span><br><span class="line">        [2., 3.]])</span><br><span class="line">tensor([[0.],</span><br><span class="line">        [1.],</span><br><span class="line">        [2.],</span><br><span class="line">        [3.]])</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure></li><li>transpose()函数可以将指定的两个维度的元素进行转置，而permute()函数则可以按照给定的维度进行维度变换。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.Tensor([[0,1],[2,3]])</span><br><span class="line">print(a)</span><br><span class="line">print(a.transpose(0,1))</span><br><span class="line">print(a.permute(1,0))</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">tensor([[0., 1.],</span><br><span class="line">        [2., 3.]])</span><br><span class="line">tensor([[0., 2.],</span><br><span class="line">        [1., 3.]])</span><br><span class="line">tensor([[0., 2.],</span><br><span class="line">        [1., 3.]])</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure></li><li>使用squeeze()与unsqueeze()函数，前者用于去除size为1的维度，而后者则是将指定的维度的size变为1。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.Tensor([[0,1],[2,3]])</span><br><span class="line">print(a)</span><br><span class="line">print(a.unsqueeze(2))</span><br><span class="line">print(a.unsqueeze(2).squeeze(2))</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">tensor([[0., 1.],</span><br><span class="line">        [2., 3.]])</span><br><span class="line">tensor([[[0.],</span><br><span class="line">         [1.]],</span><br><span class="line"></span><br><span class="line">        [[2.],</span><br><span class="line">         [3.]]])</span><br><span class="line">tensor([[0., 1.],</span><br><span class="line">        [2., 3.]])</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure></li><li>expand()函数将size为1的维度复制扩展为指定大小，也可以使用expand_as()函数指定为示例Tensor的维度。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.Tensor([[0],[3]])</span><br><span class="line">print(a)</span><br><span class="line">print(a.expand(2,4))</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">tensor([[0.],</span><br><span class="line">        [3.]])</span><br><span class="line">tensor([[0., 0., 0., 0.],</span><br><span class="line">        [3., 3., 3., 3.]])</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure></li></ol><h4 id="Tensor的排序与取极值"><a href="#Tensor的排序与取极值" class="headerlink" title="Tensor的排序与取极值"></a>Tensor的排序与取极值</h4><ol><li>函数sort()，选择沿着指定维度进行排序，返回排序后的Tensor及对应的索引位置。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.randn(3,3)</span><br><span class="line">print(a)</span><br><span class="line"># 按照第0维进行按列排序，True代表降序，False代表升序</span><br><span class="line">print(a.sort(0,True))</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">tensor([[ 0.6448,  0.5900,  2.2122],</span><br><span class="line">        [ 0.1974,  2.0291, -0.1883],</span><br><span class="line">        [-0.6540, -1.2901,  0.8186]])</span><br><span class="line">torch.return_types.sort(</span><br><span class="line">values&#x3D;tensor([[ 0.6448,  2.0291,  2.2122],</span><br><span class="line">               [ 0.1974,  0.5900,  0.8186],</span><br><span class="line">               [-0.6540, -1.2901, -0.1883]]),</span><br><span class="line">indices&#x3D;tensor([[0, 1, 0],</span><br><span class="line">                [1, 0, 2],</span><br><span class="line">                [2, 2, 1]]))</span><br></pre></td></tr></table></figure></li><li>max()与min()函数则是沿着指定维度选择最大与最小元素，返回该元素及对应的索引位置。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.randn(3,3)</span><br><span class="line">print(a)</span><br><span class="line"># 选出每一列的最大值</span><br><span class="line">print(a.max(0))</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">tensor([[ 0.8609,  2.1089,  0.5477],</span><br><span class="line">        [-0.7530,  0.6850, -0.9778],</span><br><span class="line">        [-2.0674,  0.6929,  1.4075]])</span><br><span class="line">torch.return_types.max(</span><br><span class="line">values&#x3D;tensor([0.8609, 2.1089, 1.4075]),</span><br><span class="line">indices&#x3D;tensor([0, 0, 2]))</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure><h4 id="Tensor的自动广播机制"><a href="#Tensor的自动广播机制" class="headerlink" title="Tensor的自动广播机制"></a>Tensor的自动广播机制</h4>不同形状的Tensor进行计算时，可自动扩展到较大的相同形状，再进行计算。广播机制的前提是任一个Tensor至少有一个维度，且从尾部遍历Tensor维度时，两者维度必须相等，其中一个要么是1要么不存在。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.ones(3,1,2)</span><br><span class="line">b &#x3D; torch.ones(3,1)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line"># 从尾部遍历维度，1对应3，2对应1，3对应不存在，因此满足广播条件，最后求和后的维度为[3,3,2]</span><br><span class="line">print(a+b)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">tensor([[[1., 1.]],</span><br><span class="line"></span><br><span class="line">        [[1., 1.]],</span><br><span class="line"></span><br><span class="line">        [[1., 1.]]])</span><br><span class="line">tensor([[1.],</span><br><span class="line">        [1.],</span><br><span class="line">        [1.]])</span><br><span class="line">tensor([[[2., 2.],</span><br><span class="line">         [2., 2.],</span><br><span class="line">         [2., 2.]],</span><br><span class="line"></span><br><span class="line">        [[2., 2.],</span><br><span class="line">         [2., 2.],</span><br><span class="line">         [2., 2.]],</span><br><span class="line"></span><br><span class="line">        [[2., 2.],</span><br><span class="line">         [2., 2.],</span><br><span class="line">         [2., 2.]]])</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure><h4 id="Tensor的内存共享和转换"><a href="#Tensor的内存共享和转换" class="headerlink" title="Tensor的内存共享和转换"></a>Tensor的内存共享和转换</h4></li><li>原地操作符<br>PyTorch对于一些操作通过加后缀“<em>”实现了原地操作，如add</em>()和resize_()等，这种操作只要被执行，本身的Tensor则会被改变。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.ones(2,2)</span><br><span class="line">print(a)</span><br><span class="line">a.add_(a)</span><br><span class="line">print(a)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">tensor([[1., 1.],</span><br><span class="line">        [1., 1.]])</span><br><span class="line">tensor([[2., 2.],</span><br><span class="line">        [2., 2.]])</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure></li><li>Tensor与NumPy转换<br>Tensor与NumPy可以高效地进行转换，并且转换前后的变量共享内存。在进行PyTorch不支持的操作时，甚至可以曲线救国，将Tensor转换为NumPy类型，操作后再转为Tensor。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.ones(2,2)</span><br><span class="line"># 转numpy</span><br><span class="line">b &#x3D; a.numpy()  </span><br><span class="line"># 转tensor</span><br><span class="line">c &#x3D; torch.from_numpy(b)</span><br><span class="line"># 转list</span><br><span class="line">d &#x3D; a.tolist()</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line">print(c)</span><br><span class="line">print(d)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">tensor([[1., 1.],</span><br><span class="line">        [1., 1.]])</span><br><span class="line"></span><br><span class="line">        [[1. 1.]</span><br><span class="line">        [1. 1.]]</span><br><span class="line"></span><br><span class="line">tensor([[1., 1.],</span><br><span class="line">        [1., 1.]])</span><br><span class="line"></span><br><span class="line">[[1.0, 1.0], [1.0, 1.0]]</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure></li></ol><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><blockquote><p>深度学习之PyTorch物体检测实战 - 董洪义</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensor[Pytorch][1]</title>
      <link href="/2020/08/14/DL_P6/"/>
      <url>/2020/08/14/DL_P6/</url>
      
        <content type="html"><![CDATA[<h4 id="Tensor数据类型"><a href="#Tensor数据类型" class="headerlink" title="Tensor数据类型"></a>Tensor数据类型</h4><table><thead><tr><th>数据类型</th><th>dtype</th><th>CPU tensor</th><th>GPU tensor</th></tr></thead><tbody><tr><td>32位浮点</td><td>torch.float32\torch.float</td><td>torch.FloatTensor</td><td>torch.cuda.FloatTensor</td></tr><tr><td>64位浮点</td><td>torch.float64\torch.double</td><td>torch.DoubleTensor</td><td>torch.cuda.DoubleTensor</td></tr><tr><td>16位浮点(1)</td><td>torch.float16\torch.half</td><td>torch.HalfTensor</td><td>torch.cuda.HalfTensor</td></tr><tr><td>16位浮点(2)</td><td>torch.bfloat16</td><td>torch.BFloat16Tensor</td><td>torch.cuda.BFloat16Tensor</td></tr><tr><td>32位复数</td><td>torch.complex32</td><td></td><td></td></tr><tr><td>64位复数</td><td>torch.complex64</td><td></td><td></td></tr><tr><td>128位复数</td><td>torch.complex128\torch.cdouble</td><td></td><td></td></tr><tr><td>8位无符号整型</td><td>torch.uint8</td><td>torch.ByteTensor</td><td>torch.cuda.ByteTensor</td></tr><tr><td>8位有符号整型</td><td>torch.int8</td><td>torch.CharTensor</td><td>torch.cuda.CharTensor</td></tr><tr><td>16位有符号整型</td><td>torch.int16\torch.short</td><td>torch.ShortTensor</td><td>torch.cuda.ShortTensor</td></tr><tr><td>32位有符号整型</td><td>torch.int32\torch.int</td><td>torch.IntTensor</td><td>torch.cuda.IntTensor</td></tr><tr><td>64位有符号整型</td><td>torch.int64\torch.long</td><td>torch.LongTensor</td><td>torch.cuda.LongTensor</td></tr><tr><td>布尔</td><td>torch.bool</td><td>torch.BoolTensor</td><td>torch.cuda.BoolTensor</td></tr></tbody></table><ul><li>16位半精度浮点是专为GPU上运行的模型设计的，以尽可能地节省GPU显存占用，但这种节省显存空间的方式也缩小了所能表达数据的大小。PyTorch中默认的数据类型是torch.FloatTensor，即torch.Tensor等同于torch.FloatTensor。</li><li>PyTorch可以通过set_default_tensor_type函数设置默认使用的Tensor类型，在局部使用完后如果需要其他类型，则还需要重新设置回所需的类型。<br>例：<code>torch.set_default_tensor_type(&#39;torch.DoubleTensor&#39;)</code></li></ul><h4 id="Tensor类型转换"><a href="#Tensor类型转换" class="headerlink" title="Tensor类型转换"></a>Tensor类型转换</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">#创建一个新的tensor，默认类型位32位浮点(torch.FloatTensor)</span><br><span class="line">a &#x3D; torch.Tensor(3,3)</span><br><span class="line">print(a)</span><br><span class="line">#转int类型</span><br><span class="line">b &#x3D; a.int()</span><br><span class="line">print(b)</span><br><span class="line">#转float类型</span><br><span class="line">c &#x3D; a.float()</span><br><span class="line">print(c)</span><br><span class="line">#转double类型</span><br><span class="line">d &#x3D; a.double()</span><br><span class="line">print(d)</span><br><span class="line">#转16位浮点类型</span><br><span class="line">e &#x3D; a.type(torch.HalfTensor)</span><br><span class="line">print(e)</span><br><span class="line">#转变量e的相同类型</span><br><span class="line">f &#x3D; a.type_as(e)</span><br><span class="line">print(f)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">a &#x3D; tensor([[6.3058e-44, 6.7262e-44, 7.8473e-44],</span><br><span class="line">            [6.3058e-44, 6.8664e-44, 7.2868e-44],</span><br><span class="line">            [1.1771e-43, 6.7262e-44, 7.7071e-44]])</span><br><span class="line">b &#x3D; tensor([[0, 0, 0],</span><br><span class="line">            [0, 0, 0],</span><br><span class="line">            [0, 0, 0]], dtype&#x3D;torch.int32)</span><br><span class="line">c &#x3D; tensor([[6.3058e-44, 6.7262e-44, 7.8473e-44],</span><br><span class="line">            [6.3058e-44, 6.8664e-44, 7.2868e-44],</span><br><span class="line">            [1.1771e-43, 6.7262e-44, 7.7071e-44]])</span><br><span class="line">d &#x3D; tensor([[6.3058e-44, 6.7262e-44, 7.8473e-44],</span><br><span class="line">            [6.3058e-44, 6.8664e-44, 7.2868e-44],</span><br><span class="line">            [1.1771e-43, 6.7262e-44, 7.7071e-44]], dtype&#x3D;torch.float64)</span><br><span class="line">e &#x3D; tensor([[0., 0., 0.],</span><br><span class="line">            [0., 0., 0.],</span><br><span class="line">            [0., 0., 0.]], dtype&#x3D;torch.float16)</span><br><span class="line">f &#x3D; tensor([[0., 0., 0.],</span><br><span class="line">            [0., 0., 0.],</span><br><span class="line">            [0., 0., 0.]], dtype&#x3D;torch.float16)</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure><h4 id="Tensor的创建与维度查看"><a href="#Tensor的创建与维度查看" class="headerlink" title="Tensor的创建与维度查看"></a>Tensor的创建与维度查看</h4><ol><li>常规创建<br>例：<code>a = torch.Tensor(2,2)</code></li><li>指定类型创建<br>例：<code>a = torch.HalfTensor(2,2)</code></li><li>直接使用Python的list创建<br>例：<code>a = torch.Tensor([[2,2],[3,3]])</code></li><li>创建一个元素全为0的tensor<br>例：<code>a = torch.zeros(2, 2)</code></li><li>创建一个元素全为1的tensor<br>例：<code>a = torch.ones(2, 2)</code></li><li>创建一个对角线元素全为1的<strong>2维矩阵</strong>，行列可以不相同，可用于构造单位矩阵<br>例：<code>a = torch.eye(2, 2)</code></li><li>创建一个随机tensor<br>例：<code>a = torch.randon(2, 2)</code></li><li>创建一个随机排列的<strong>1维向量</strong><br>例：<code>a = torch.randperm(n)#长度为n</code></li><li><code>torch.arange(start, end, step)</code><br>创建一个从start到end，间距为step，<strong>1维向量</strong></li><li><code>torch.linspace(start, end, step)</code><br>创建一个从start到end，一共steps份，<strong>1维向量</strong></li><li>查看tensor的维度<br>例：<code>a.shape</code> 或 <code>a.size()</code></li><li>查看tensor的元素总个数<br>例：<code>a.numel()</code> 或 <code>a.nelement()</code><h4 id="Tensor的组合与分块"><a href="#Tensor的组合与分块" class="headerlink" title="Tensor的组合与分块"></a>Tensor的组合与分块</h4></li><li><code>torch.cat()</code><br>沿着已有的数据的某一维度进行拼接，操作后数据的总维数不变，在进行拼接时，除了拼接的维度之外，其他维度必须相同<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.Tensor([[2,2],[3,3]])</span><br><span class="line">b &#x3D; torch.Tensor([[4,4],[5,5]])</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line">c &#x3D; torch.cat([a,b], 0) #沿着第1维拼接</span><br><span class="line">print(c)</span><br><span class="line">print(c.size())</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">a &#x3D; tensor([[2., 2.],</span><br><span class="line">            [3., 3.]])</span><br><span class="line">b &#x3D; tensor([[4., 4.],</span><br><span class="line">            [5., 5.]])</span><br><span class="line">c &#x3D; tensor([[2., 2.],</span><br><span class="line">            [3., 3.],</span><br><span class="line">            [4., 4.],</span><br><span class="line">            [5., 5.]])</span><br><span class="line">c.size &#x3D; torch.Size([4, 2])</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure></li><li><code>torch.stack()</code><br>新增维度，并按照指定的维度进行叠加<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.Tensor([[2,2],[3,3]])</span><br><span class="line">b &#x3D; torch.Tensor([[4,4],[5,5]])</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line">d &#x3D; torch.stack([a,b],1) #沿着第1维拼接</span><br><span class="line">print(d)</span><br><span class="line">print(d.size())</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">a &#x3D; tensor([[2., 2.],</span><br><span class="line">            [3., 3.]])</span><br><span class="line">b &#x3D; tensor([[4., 4.],</span><br><span class="line">            [5., 5.]])</span><br><span class="line">d &#x3D; tensor([[[2., 2.],</span><br><span class="line">            [4., 4.]],</span><br><span class="line"></span><br><span class="line">            [[3., 3.],</span><br><span class="line">             [5., 5.]]])</span><br><span class="line">torch.Size([2, 2, 2])</span><br></pre></td></tr></table></figure></li><li><code>torch.chunk()</code><br>将Tensor分割成不同的子Tensor，需要指定分块的数量<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.Tensor([[2,3,4],[2,3,4]])</span><br><span class="line">print(a)</span><br><span class="line"># 沿着第1维进行分块，因此分割成两个Tensor，当不能整除时，最后一个的维数会小于前面的</span><br><span class="line"># 因此第一个Tensor为2×2，第二个为2×1</span><br><span class="line">d &#x3D; torch.chunk(a,2,1)</span><br><span class="line">print(d)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result：</span><br><span class="line"></span><br><span class="line">a &#x3D; tensor([[2., 3., 4.],</span><br><span class="line">            [2., 3., 4.]])</span><br><span class="line">d &#x3D; (tensor([[2., 3.],</span><br><span class="line">             [2., 3.]]), tensor([[4.],</span><br><span class="line">                                [4.]]))</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure></li><li><code>torch.split()</code><br>将Tensor分割成不同的子Tensor，需要指定每一块的大小，以整型或者list来表示<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.Tensor([[2,3,4],[2,3,4]])</span><br><span class="line">print(a)</span><br><span class="line"># 沿着第1维分块，每一块维度为2，因此第一个Tensor为2×2，第二个为2×1</span><br><span class="line">b &#x3D; torch.split(a,2,1)</span><br><span class="line">print(b)</span><br><span class="line"># split也可以根据输入的list进行自动分块，list中的元素代表了每一个块占的维度</span><br><span class="line">c &#x3D; torch.split(a,[1,2],1)</span><br><span class="line">print(c)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">a &#x3D; tensor([[2., 3., 4.],</span><br><span class="line">            [2., 3., 4.]])</span><br><span class="line">b &#x3D; (tensor([[2., 3.],</span><br><span class="line">             [2., 3.]]), tensor([[4.],</span><br><span class="line">                                [4.]]))</span><br><span class="line">c &#x3D; (tensor([[2.],</span><br><span class="line">             [2.]]), tensor([[3., 4.],</span><br><span class="line">                             [3., 4.]]))</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于深度学习的Python基础知识</title>
      <link href="/2020/08/07/DL_P5/"/>
      <url>/2020/08/07/DL_P5/</url>
      
        <content type="html"><![CDATA[<h4 id="不可变对象和可变对象"><a href="#不可变对象和可变对象" class="headerlink" title="不可变对象和可变对象"></a>不可变对象和可变对象</h4><ul><li>不可变对象：对象对应内存中的值不会变，因此如果指向该对象的变量被改变了，Pyhton则会重新开辟一片内存，变量再指向这个新的内存，包括int、float、str、tuple(元组)等。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 1</span><br><span class="line">b &#x3D; a</span><br><span class="line">b &#x3D; 2</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"># result: a &#x3D; 1</span><br></pre></td></tr></table></figure></li><li>可变对象：对象对应内存中的值可以改变，因此变量改变后，该对象也会改变，即原地修改，如list、dictionary、set等。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; [1]</span><br><span class="line">b &#x3D; a</span><br><span class="line">b.append(2)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"># result: a &#x3D; [1,2]</span><br></pre></td></tr></table></figure></li></ul><p><strong>注：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; [1]</span><br><span class="line">b &#x3D; a</span><br><span class="line">b &#x3D; [1,2]</span><br><span class="line">print(a)</span><br><span class="line"># result: a &#x3D; [1]</span><br></pre></td></tr></table></figure><p>这里的变量a没有发生任何变化因为变量b指向了一个新的内存，详见下列代码进行对比：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; [1]</span><br><span class="line">b &#x3D; a</span><br><span class="line">print(id(a)&#x3D;&#x3D;id(b))</span><br><span class="line">b.append(2)</span><br><span class="line">print(id(a)&#x3D;&#x3D;id(b))</span><br><span class="line"># result: True,True</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; [1]</span><br><span class="line">b &#x3D; a</span><br><span class="line">print(id(a)&#x3D;&#x3D;id(b))</span><br><span class="line">b &#x3D; [1,2]</span><br><span class="line">print(id(a)&#x3D;&#x3D;id(b))</span><br><span class="line"></span><br><span class="line"># result: True,False</span><br></pre></td></tr></table></figure><h4 id="浅拷贝和深拷贝"><a href="#浅拷贝和深拷贝" class="headerlink" title="浅拷贝和深拷贝"></a>浅拷贝和深拷贝</h4><ul><li>浅拷贝：使用copy()函数，拷贝了list最外围，而list内部的对象仍然是引用。</li><li>深拷贝：使用deepcopy()函数，list内外围均为拷贝，因此前后的变量完全隔离，而非引用。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import copy</span><br><span class="line">a &#x3D; [1,2,3,[1,2,3]]</span><br><span class="line">b &#x3D; a                   #变量内外部都是引用</span><br><span class="line">c &#x3D; copy.copy(a)        #浅拷贝，内部是引用</span><br><span class="line">d &#x3D; a[:]                #同上</span><br><span class="line">e &#x3D; copy.deepcopy(a)    #深拷贝，内外部都完全隔离不是引用</span><br><span class="line">a.append(4) </span><br><span class="line">a[3].append(4)</span><br><span class="line">print(b)</span><br><span class="line">print(c)</span><br><span class="line">print(d)</span><br><span class="line">print(e)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line">[1, 2, 3, [1, 2, 3, 4], 4]</span><br><span class="line">[1, 2, 3, [1, 2, 3, 4]]</span><br><span class="line">[1, 2, 3, [1, 2, 3, 4]]</span><br><span class="line">[1, 2, 3, [1, 2, 3]]</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure></li></ul><h4 id="局部变量和全局变量"><a href="#局部变量和全局变量" class="headerlink" title="局部变量和全局变量"></a>局部变量和全局变量</h4><ul><li>局部变量是无法修改全局变量的。想要实现局部修改全局变量，通常有两种办法，增加globa等关键字，或者使用list和dict等可变对象的内置函数。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 1 </span><br><span class="line">def local(): </span><br><span class="line">      a &#x3D; 2     #局部变量，可以看做为新的变量</span><br><span class="line">local()</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"># result: 1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 1 </span><br><span class="line">b &#x3D; [1]</span><br><span class="line">def local(): </span><br><span class="line">      global a                  #使用global关键字，表明在局部使用的是全局的a变量</span><br><span class="line">      a &#x3D; 2 </span><br><span class="line">      b.append(2)               #对于可变对象，使用内置函数则会修改全局变量</span><br><span class="line">local()</span><br><span class="line">print(a)                        </span><br><span class="line">print(b)                        </span><br><span class="line"></span><br><span class="line"># result: 2,[1,2]</span><br></pre></td></tr></table></figure></li></ul><h4 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h4><ul><li>abs(): 返回数字的绝对值<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(abs(-1))</span><br><span class="line"></span><br><span class="line"># result: 1</span><br></pre></td></tr></table></figure></li><li>map(): 可以将一个函数映射作用到可迭代的序列中，并返回函数输出的序列<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def f(x):</span><br><span class="line">    return x + 1</span><br><span class="line">result &#x3D; list(map(f,[1,2,3])) #python3中输出前需要转化为list</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"># reuslt: [2, 3, 4]</span><br></pre></td></tr></table></figure></li><li>reduce(): 输入的函数需要传入两个参数。reduce()的过程是先使用输入函数对序列中的前两个元素进行操作，得到的结果再和第三个元素进行运算，直到最后一个元素。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce  </span><br><span class="line">def f(x, y):</span><br><span class="line">    return x*10+y</span><br><span class="line">result &#x3D; reduce(f, [1, 2, 3, 4])</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">#result: 1234</span><br></pre></td></tr></table></figure></li><li>filter(): 通过输入函数对可迭代序列进行过滤，并返回满足过滤条件的可迭代序列<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def is_odd(n): </span><br><span class="line">    return n % 2 &#x3D;&#x3D; 0</span><br><span class="line">result &#x3D; list(filter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15]))</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">#result: [2, 4, 6, 10]</span><br></pre></td></tr></table></figure></li><li>sorted(): 函数可以完成对可迭代序列的排序。与列表本身自带的sort()函数不同，这里的sorted()函数返回的是一个新的列表。sorted()函数可以传入关键字key来指定排序的标准，参数reverse代表是否反向。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result &#x3D; list(sorted([3, 5, -87, 0, -21], key&#x3D;abs, reverse&#x3D;True))  # 绝对值排序，并且为反序</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">#result: [-87, -21, 5, 3, 0]</span><br></pre></td></tr></table></figure></li><li>对于一些简单逻辑函数，可以使用lambda匿名表达式来取代函数的定义，类似于def<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">add &#x3D; lambda x,y: x + y</span><br><span class="line">print(add(1,2))</span><br></pre></td></tr></table></figure></li></ul><h4 id="迭代器与生成器"><a href="#迭代器与生成器" class="headerlink" title="迭代器与生成器"></a>迭代器与生成器</h4><ul><li>迭代器(iterator): 不要求事先准备好整个迭代过程中所有的元素，可以使用next()来访问元素。Python中的容器，如list、dict和set等，都属于可迭代对象，对于这些容器，我们可以使用iter()函数封装成迭代器。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; [1,2,3,4,5,6]</span><br><span class="line">y &#x3D; iter(x)</span><br><span class="line">z &#x3D; iter(x)</span><br><span class="line">print(next(y),next(z))</span><br><span class="line"></span><br><span class="line">#result: 1 1</span><br></pre></td></tr></table></figure>上面的例子证明了迭代器之间相互独立</li><li>生成器(generator):是迭代器的一种，可以控制循环遍历的过程，实现一边循环一边计算，并使用yield来返回函数值，每次调用到yield会暂停。生成器迭代的序列可以不是完整的，从而可以节省出大量的内存空间。<br>例：斐波那契数列<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def fibonacci():</span><br><span class="line">    a &#x3D; [1, 1]</span><br><span class="line">    while True:</span><br><span class="line">            a.append(sum(a))        # 往列表里添加下一个元素</span><br><span class="line">            yield a.pop(0)          # 取出第0个元素，并停留在当前执行点</span><br><span class="line">result &#x3D; []</span><br><span class="line">for x in fibonacci():</span><br><span class="line">    if x &gt; 10:    </span><br><span class="line">            break                   # 仅打印小于10的数字</span><br><span class="line">    result.append(x)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">#result: [1, 1, 2, 3, 5, 8]</span><br></pre></td></tr></table></figure></li><li>还可以使用”()”创建生成器<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; (x for x in range(1,10))</span><br><span class="line">print(next(a))</span><br><span class="line"></span><br><span class="line">#result: 1</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于矩阵，向量和标量的转置，相加和相乘</title>
      <link href="/2020/08/05/DL_Theory2/"/>
      <url>/2020/08/05/DL_Theory2/</url>
      
        <content type="html"><![CDATA[<h4 id="转置-transpose"><a href="#转置-transpose" class="headerlink" title="转置(transpose)"></a>转置(transpose)</h4><ul><li>定义：给定m x n矩阵A，则A的转置是一个n x m矩阵，它的列是由A的行组成的</li><li>变量表示：<br>$$ \bm{A}^T $$</li><li>例：给定一个矩阵：<br>$$ \bm{A} = \begin{bmatrix} A_{1,1} &amp; A_{1,2} \cr A_{2,1} &amp; A_{2,2} \end{bmatrix} $$<br>它的转置是：<br>$$ \bm{A}^{T} = \begin{bmatrix} A_{1,1} &amp; A_{2,1} \cr A_{1,2} &amp; A_{2,2} \end{bmatrix} $$</li><li>有时，我们通过将向量元素作为行矩阵写在文本行中，然后使用转置操作将其变为标准的列向量，比如：<br>$$ \bm{x} = [x_1, x_2, x_3]^T $$</li><li>标量的转置等于它本身，即：<br>$$ a = a^T $$</li></ul><h4 id="加法"><a href="#加法" class="headerlink" title="加法"></a>加法</h4><ul><li>定义：只要矩阵的形状一样，我们可以把两个矩阵相加。两个矩阵相加是指对应位置的元素相加，比如：<br>$$ \bm{C} = \bm{A} + \bm{B} $$ 其中 $$ C_{i,j} = A_{i,j} + B_{i,j} $$</li><li>标量和矩阵相加时，我们只需要将其与矩阵的每个元素相加，比如<br>$$ \bm{D} = \bm{B} + c $$ 其中 $$ D_{i,j} = B_{i,j} + c $$</li><li>在深度学习中，我们允许矩阵和向量相加，比如：<br>$$ \bm{C} = \bm{A} + \bm{b} $$ 其中 $$ C_{i,j} = A_{i,j} + b_{j} $$<br>这种隐式地复制向量b到很多位置的方式，称为<strong>广播(broadcasting)</strong></li></ul><h4 id="乘法"><a href="#乘法" class="headerlink" title="乘法"></a>乘法</h4><ul><li>定义：两个矩阵A和B的<strong>矩阵乘积(matrix product)</strong>是第三个矩阵C(n x p)。为了使乘法可被定义，<em>矩阵A(m x n)的列数</em>必须和<em>矩阵B(n x p)的行数</em>相等</li><li>写法：$$ \bm{C} = \bm{A}  \bm{B} $$<br>具体操作方法：<br>$$ C_{i,j} = \displaystyle\sum_{k} A_{i,k} B_{k,j} $$</li></ul><h4 id="参考目录"><a href="#参考目录" class="headerlink" title="参考目录"></a>参考目录</h4><blockquote><p>Goodfellow, I., Bengio, Y. and Courville, A., 2016. Deep learning. MIT press.<br>Lay, D.C., 2016. Linear Algebra and its applications 5th edition. Pearson.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[理论] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>标量、向量、矩阵和张量</title>
      <link href="/2020/08/02/DL_Theory1/"/>
      <url>/2020/08/02/DL_Theory1/</url>
      
        <content type="html"><![CDATA[<h4 id="标量-scalar"><a href="#标量-scalar" class="headerlink" title="标量(scalar)"></a>标量(scalar)</h4><ul><li>定义：一个标量就是一个单独的数</li><li>变量表示：斜体，小写</li><li>例：一个自然数标量<br>$$ n\in\mathbb{N} $$</li></ul><h4 id="向量-vector"><a href="#向量-vector" class="headerlink" title="向量(vector)"></a>向量(vector)</h4><ul><li>定义：一个向量是一列数。这些数是有序排列的。如果向量中的每个元素都属于实数R，并且该向量有n个元素，则记为：<br>$$ \mathbb{R}^n $$</li><li>变量表示：粗体，小写</li><li>例：一个有n个元素的列向量<br>$$ \bm{x} = \begin{bmatrix} x_1 \cr x_2 \cr \vdots \cr x_n \end{bmatrix} $$</li><li>我们用符号“ - ” 表示集合的补集中的索引。例：<br>$$ 指定 x_1, x_3, x_6, 我们定义集合 S = \lbrace 1,3,6  \rbrace,然后写作\bm{x}_S $$</li></ul><p>$$ \bm{x}_{-1}表示\bm{x}中除x_1外的所有元素所构成的向量 $$</p><p>$$ \bm{x}_{-S}表示\bm{x}中除x_1, x_3, x_6外的所有元素所构成的向量 $$</p><h4 id="矩阵-matrix"><a href="#矩阵-matrix" class="headerlink" title="矩阵(matrix)"></a>矩阵(matrix)</h4><ul><li>定义：是一个二维数组</li><li>变量表示：粗体，大写。在表示矩阵中的元素时：斜体，大写</li><li>例：一个2X2矩阵<br>$$ \bm{A} = \begin{bmatrix} A_{1,1} &amp; A_{1,2} \cr A_{2,1} &amp; A_{2,2} \end{bmatrix} $$</li><li>通过用“ ：”表示水平坐标，以表示垂直坐标i中的所有元素。</li><li>例：矩阵的第i行(row)：<br>$$ \bm{A}_{i,:} $$</li><li>例：矩阵的第i列(column)：<br>$$ \bm{A}_{:,i} $$</li><li>表示函数f作用在矩阵上输出的矩阵的第i行第j列元素：<br>$$ f(\bm{A})_{i,j} $$</li></ul><h4 id="张量-tensor"><a href="#张量-tensor" class="headerlink" title="张量(tensor)"></a>张量(tensor)</h4><ul><li>定义：坐标超过两维的数组。一般的，一个数组中的元素分布在若干维坐标的规则网络中，我们称之为张量。</li><li>变量表示：$$ \textsf{\textbf{A}} $$</li></ul><h4 id="参考目录"><a href="#参考目录" class="headerlink" title="参考目录"></a>参考目录</h4><blockquote><p>Goodfellow, I., Bengio, Y. and Courville, A., 2016. Deep learning. MIT press.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[理论] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模型评估测试[分类问题]</title>
      <link href="/2020/07/28/DL_P4/"/>
      <url>/2020/07/28/DL_P4/</url>
      
        <content type="html"><![CDATA[<h4 id="第一步：准备测试图片和测试模型"><a href="#第一步：准备测试图片和测试模型" class="headerlink" title="第一步：准备测试图片和测试模型"></a>第一步：准备测试图片和测试模型</h4><p>准备好一张需要测试的图片和训练好的测试模型，放到测试文件夹中</p><h4 id="第二步：测试代码"><a href="#第二步：测试代码" class="headerlink" title="第二步：测试代码"></a>第二步：测试代码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from PIL import Image</span><br><span class="line">from torchvision import transforms</span><br><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">#加载测试模型</span><br><span class="line">model &#x3D; torch.load(&#39;C:&#x2F;Users&#x2F;Arthur&#x2F;Desktop&#x2F;test&#x2F;model.pth&#39;)</span><br><span class="line">#加载测试图片</span><br><span class="line">test_img &#x3D; Image.open(&#39;C:&#x2F;Users&#x2F;Arthur&#x2F;Desktop&#x2F;test&#x2F;12.png&#39;)</span><br><span class="line">#图片变换</span><br><span class="line">transform &#x3D; transforms.Compose([transforms.Resize((224,224))</span><br><span class="line">                                       ,transforms.ToTensor()</span><br><span class="line">                                       ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])</span><br><span class="line">test_img &#x3D; transform(test_img)</span><br><span class="line">#增加一维度</span><br><span class="line">inputs &#x3D;Variable(torch.unsqueeze(test_img, dim&#x3D;0).float(), requires_grad&#x3D;False).cuda()</span><br><span class="line">outputs &#x3D; model(inputs)</span><br><span class="line">#经过sofamax层后输出概率</span><br><span class="line">probability &#x3D; torch.nn.functional.softmax(outputs,dim&#x3D;1)</span><br><span class="line">#找出最大概率和相应的标签</span><br><span class="line">max_value,index &#x3D; torch.max(probability,1)</span><br><span class="line">print(probability,max_value,index)</span><br></pre></td></tr></table></figure><p><strong>注：</strong></p><ol><li>测试图片的尺寸需要和训练尺寸相同，并且也需要进行张量变换（ToTensor）和归一化（Normalize）。</li><li>由于pytorch要求的输入的维度为[batch_size, channels, width, height]，而一个样本的维度为[channels, width, height]，此时因为我们需要测试一张图片，所以用unsqueeze()增加一个维度变为[1, channels, width, height]。</li><li>index只能给你一个数字，它其实代表的是第几个类别，如果想知道具体类别名称，可以打开训练集的文件夹，那么0就是第一个类别，1就是第二个类别…….以此类推。比如以下是我的训练集。<br><img src="https://cdn.jsdelivr.net/gh/xq14183903/xq14183903.github.io@latest/images/dl_practice/4/1.png" alt="" loading="lazy"><em>训练集文件夹</em><br>那么mn就是第0个类别，mp就是第一个类别。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据集图片变换与增强[transform][augmentation]</title>
      <link href="/2020/07/25/DL_P3/"/>
      <url>/2020/07/25/DL_P3/</url>
      
        <content type="html"><![CDATA[<h3 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h3><p>所有数据集图片的格式必须要求被<strong>PIL</strong>所支持。<br>详细信息请见此篇文章：</p><blockquote><p><a href="https://xq14183903.github.io/2020/07/24/DL_P2/" target="_blank" rel="noopener">https://xq14183903.github.io/2020/07/24/DL_P2/</a></p></blockquote><h3 id="图片变换与增强"><a href="#图片变换与增强" class="headerlink" title="图片变换与增强"></a>图片变换与增强</h3><p>官方文档：</p><blockquote><p><a href="https://pytorch.org/docs/stable/torchvision/transforms.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torchvision/transforms.html</a></p></blockquote><h4 id="CenterCrop（中心裁剪）"><a href="#CenterCrop（中心裁剪）" class="headerlink" title="CenterCrop（中心裁剪）"></a>CenterCrop（中心裁剪）</h4><p><code>transforms.CenterCrop(size)</code><br>size：(高,宽)或(边长)。当输入的是一组高和宽时，图片以中心为原点，设定的高和宽为基础，裁剪图片。当输入的size是一个边长时，裁剪的图片必定为正方形</p><h4 id="ColorJitter（亮度、对比度、饱和度、色调）"><a href="#ColorJitter（亮度、对比度、饱和度、色调）" class="headerlink" title="ColorJitter（亮度、对比度、饱和度、色调）"></a>ColorJitter（亮度、对比度、饱和度、色调）</h4><p><code>transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)</code><br>brightness：（min，max），输入必须为非负数。<br>contrast：同上<br>saturation：同上<br>hue：（min，max），且 -0.5 &lt;= min &lt;= max &lt;= 0.5</p><h4 id="Grayscale（灰度化）"><a href="#Grayscale（灰度化）" class="headerlink" title="Grayscale（灰度化）"></a>Grayscale（灰度化）</h4><p><code>transforms.Grayscale(num_output_channels=1)</code><br>num_output_channels：如果等于1，那么输出的单通道图片。如果等于3，那么输出的是RGB图片</p><h4 id="Pad（填充）"><a href="#Pad（填充）" class="headerlink" title="Pad（填充）"></a>Pad（填充）</h4><p><code>transforms.Pad(padding, fill=0, padding_mode=&#39;constant&#39;)</code><br>padding：（int）填充上下左右四边。（num1，num2）num1对应填充左和右，num2对应填充上和下。（num1，num2，num3，num4），num1、num2、num3、num4分别对应左上右下。<br>fill：(R,G,B) <strong>只能在padding_mode=’constant’时使用</strong>。用于选择填充颜色变换。<br>padding_mode：</p><ul><li>constant：填充为fill设置好的颜色</li><li>edge：根据图片边缘最后一个值进行填充</li><li>reflect：根据图片的反射来填充图片，且不重复图片边缘最后一个值</li><li>symmetric：根据图片的反射来填充图片，且重复图片边缘最后一个值 （对称填充）</li></ul><h4 id="RandomAffine（随机仿射变换）"><a href="#RandomAffine（随机仿射变换）" class="headerlink" title="RandomAffine（随机仿射变换）"></a>RandomAffine（随机仿射变换）</h4><p><code>transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)</code><br> degrees：如果输入的是个int，那么变换角度为-int ~ +int，若输入是个元组（min，max），故变换范围为-min ~ +max<br> translate：（num1，num2），图像进行水平或垂直移动。计算公式为 -img_width * a &lt; dx &lt; img_width * a 和<br> -img_height * b &lt; dy &lt; img_height * b。<br> shear：平行于x轴剪裁，也是有点类似中心放大的感觉。三种种输入可选，（int）（num1，num2）（num1，num2，num3，num4）。前两种的裁剪范围是 -int ~ +int 和 num1 ~ num2，最后一种有点特殊，（num1，num2）是平行于x轴剪裁，（num3，num4）再组合成为另外一个范围，用于平行于y轴剪裁。<br>resample：重采样方式，支持 PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC。<br>fillcolor：和transforms.Pad的一样。</p><h4 id="RandomApply（随机变换应用）"><a href="#RandomApply（随机变换应用）" class="headerlink" title="RandomApply（随机变换应用）"></a>RandomApply（随机变换应用）</h4><p><code>transforms.RandomApply(transforms, p=0.5)</code><br>transforms：使用元组或列表把需要随机应用的变换组合在一起，如 transforms = [transforms.RandomAffine(30, translate=(0,0), scale=(1,2), shear=(1,10,3,4), resample=Image.NEAREST, fillcolor=255)]，可以添加多个。<br>p：应用概率</p><h4 id="RandomChoice（随机变换应用选择）"><a href="#RandomChoice（随机变换应用选择）" class="headerlink" title="RandomChoice（随机变换应用选择）"></a>RandomChoice（随机变换应用选择）</h4><p><code>transforms.RandomChoice(transforms)</code><br>transforms：和RandomApply一样，效果是在设定好的元组或列表里选择一个变换进行应用</p><h4 id="RandomCrop（随机裁剪）"><a href="#RandomCrop（随机裁剪）" class="headerlink" title="RandomCrop（随机裁剪）"></a>RandomCrop（随机裁剪）</h4><p><code>torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode=&#39;constant&#39;)</code><br>size：(高,宽)或(边长)。当输入的是一组高和宽时，图片以中心为原点，设定的高和宽为基础，裁剪图片。当输入的size是一个边长时，裁剪的图片必定为正方形。<br>padding：（int）填充上下左右四边。（num1，num2）num1对应填充左和右，num2对应填充上和下。（num1，num2，num3，num4），num1、num2、num3、num4分别对应左上右下。<br>pad_if_needed：如果裁剪后的图片小于预期值，那么会进行自动填充。<br>fill：(R,G,B) <strong>只能在padding_mode=’constant’时使用</strong>。用于选择填充颜色变换。<br>padding_mode：</p><ul><li>constant：填充为fill设置好的颜色</li><li>edge：根据图片边缘最后一个值进行填充</li><li>reflect：根据图片的反射来填充图片，且不重复图片边缘最后一个值</li><li>symmetric：根据图片的反射来填充图片，且重复图片边缘最后一个值 （对称填充）</li></ul><h4 id="RandomGrayscale（随机灰度化）"><a href="#RandomGrayscale（随机灰度化）" class="headerlink" title="RandomGrayscale（随机灰度化）"></a>RandomGrayscale（随机灰度化）</h4><p><code>transforms.RandomGrayscale(p=0.1)</code><br>p：随机灰度化图片的概率</p><h4 id="RandomGrayscale（随机水平翻折）"><a href="#RandomGrayscale（随机水平翻折）" class="headerlink" title="RandomGrayscale（随机水平翻折）"></a>RandomGrayscale（随机水平翻折）</h4><p><code>transforms.RandomHorizontalFlip(p=0.5)</code><br>p：随机灰度化图片的概率</p><h4 id="RandomOrder（随机变换应用顺序）"><a href="#RandomOrder（随机变换应用顺序）" class="headerlink" title="RandomOrder（随机变换应用顺序）"></a>RandomOrder（随机变换应用顺序）</h4><p><code>transforms.RandomOrder(transforms)</code><br>如果列表里有[1,2,3,4]4种变换，随机进行排序应用。<br>transforms：使用元组或列表把需要随机应用的变换组合在一起，如 transforms = [transforms.RandomAffine(30, translate=(0,0), scale=(1,2), shear=(1,10,3,4), resample=Image.NEAREST, fillcolor=255)]，可以添加多个。</p><h4 id="RandomPerspective（随机透视变换）"><a href="#RandomPerspective（随机透视变换）" class="headerlink" title="RandomPerspective（随机透视变换）"></a>RandomPerspective（随机透视变换）</h4><p><code>transforms.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=3, fill=0)</code><br>interpolation：Default- Image.BICUBIC（不懂，没用过，直接上官网原版，后续用到了再补）<br>p：随机灰度化图片的概率<br>distortion_scale：形变范围，取值范围是[0,1]，默认为0.5<br>fill：(R,G,B) ，用于选择填充颜色变换。</p><h4 id="RandomResizedCrop（随机调整图像大小裁剪）"><a href="#RandomResizedCrop（随机调整图像大小裁剪）" class="headerlink" title="RandomResizedCrop（随机调整图像大小裁剪）"></a>RandomResizedCrop（随机调整图像大小裁剪）</h4><p><code>transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)</code><br>size：(高,宽)或(边长)。当输入的是一组高和宽时，图片以中心为原点，设定的高和宽为基础，裁剪图片。当输入的size是一个边长时，裁剪的图片必定为正方形。<br>scale：原图尺寸裁剪范围，默认为（0.08，1.0）<br>ratio：原图长宽比裁剪范围，默认为（3/4，4/3）<br>interpolation： Default- PIL.Image.BILINEAR</p><h4 id="RandomRotation（随机旋转）"><a href="#RandomRotation（随机旋转）" class="headerlink" title="RandomRotation（随机旋转）"></a>RandomRotation（随机旋转）</h4><p><code>transforms.RandomRotation(degrees, resample=False, expand=False, center=None, fill=None)</code><br>degrees：如果输入的是个int，那么变换角度为-int ~ +int，若输入是个元组（min，max），故变换范围为-min ~ +max<br>resample：重采样方式，支持 PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC。<br>expand：true，确保旋转后的图片能全部显示在原尺寸。false，旋转后可能丢失些图片边缘信息。<br>center：（x，y）确定旋转中心。默认为图片中心。<br>fill：(R,G,B) ，用于选择填充颜色变换。</p><h4 id="RandomVerticalFlip（随机垂直翻转）"><a href="#RandomVerticalFlip（随机垂直翻转）" class="headerlink" title="RandomVerticalFlip（随机垂直翻转）"></a>RandomVerticalFlip（随机垂直翻转）</h4><p><code>transforms.RandomVerticalFlip(p=0.5)</code><br>p：随机灰度化图片的概率</p><h4 id="Resize（修剪图片尺寸）"><a href="#Resize（修剪图片尺寸）" class="headerlink" title="Resize（修剪图片尺寸）"></a>Resize（修剪图片尺寸）</h4><p><code>transforms.Resize(size, interpolation=2)</code><br>size：(高,宽)或(边长)。当输入的是一组高和宽时，图片以中心为原点，设定的高和宽为基础，裁剪图片。当输入的size是一个边长时，裁剪的图片必定为正方形。<br>interpolation： Default- PIL.Image.BILINEAR</p><h4 id="LinearTransformation（线性变换）"><a href="#LinearTransformation（线性变换）" class="headerlink" title="LinearTransformation（线性变换）"></a>LinearTransformation（线性变换）</h4><p>transforms.LinearTransformation(transformation_matrix, mean_vector)<br>没用过，直接翻译官方应用：白化变换：假设X是一个列向量并中心为0的数据，然后计算该数据的协方差矩阵[D x D]（使用<br> torch.mm(X.t(), X)），计算出来的协方差矩阵再进行奇异值分解然后传入transformation_matrix。<br>mean_vector：平均向量</p><h4 id="Normalize（归一化）"><a href="#Normalize（归一化）" class="headerlink" title="Normalize（归一化）"></a>Normalize（归一化）</h4><p><code>transforms.Normalize(mean, std, inplace=False)</code><br>mean：每个通道的均值<br>std：每个通道的标准差<br>通常为：<code>transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</code></p><h4 id="RandomErasing（随机擦除）"><a href="#RandomErasing（随机擦除）" class="headerlink" title="RandomErasing（随机擦除）"></a>RandomErasing（随机擦除）</h4><p><code>transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)</code><br>p：随机灰度化图片的概率<br>scale：擦除比率范围<br>ratio：擦除长宽高比率范围<br>value：擦除后填补的颜色，同样使用（R，G，B）模式<br>通常放在transforms.Normalize后面</p><h4 id="ToPILImage（PIL图片格式转换）"><a href="#ToPILImage（PIL图片格式转换）" class="headerlink" title="ToPILImage（PIL图片格式转换）"></a>ToPILImage（PIL图片格式转换）</h4><p><code>transforms.ToPILImage(mode=None)</code><br>把一个tensor或narray转换为PIL图片<br>mode：如果输入有4通道，那么建议为RGBA，3通道-RGB，2通道-LA，单通道由数据类型决定，如int、float、short</p><h4 id="ToTensor（张量格式转换）"><a href="#ToTensor（张量格式转换）" class="headerlink" title="ToTensor（张量格式转换）"></a>ToTensor（张量格式转换）</h4><p><code>transforms.ToTensor()</code></p><h4 id="Lambda（应用用户自定义的变换）"><a href="#Lambda（应用用户自定义的变换）" class="headerlink" title="Lambda（应用用户自定义的变换）"></a>Lambda（应用用户自定义的变换）</h4><p><code>transforms.Lambda(lambd)</code></p><h4 id="Compose（存放各种变换应用）"><a href="#Compose（存放各种变换应用）" class="headerlink" title="Compose（存放各种变换应用）"></a>Compose（存放各种变换应用）</h4><p><code>transforms.Compose([])</code></p><h3 id="效果可视化"><a href="#效果可视化" class="headerlink" title="效果可视化"></a>效果可视化</h3><p>完整代码(例)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from PIL import Image</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from torchvision import transforms</span><br><span class="line">from torchvision.datasets import ImageFolder</span><br><span class="line"></span><br><span class="line">defined_transform &#x3D; transforms.Compose([transforms.Resize((32,32)),</span><br><span class="line">                                       transforms.RandomHorizontalFlip(p&#x3D;0.5),</span><br><span class="line">                                       transforms.RandomRotation(30),</span><br><span class="line">                                        transforms.ToTensor(),</span><br><span class="line">                                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])</span><br><span class="line">train &#x3D; ImageFolder(&#39;C:&#x2F;Users&#x2F;Arthur&#x2F;Desktop&#x2F;sum&#x2F;tempu&#x2F;train&#x2F;&#39;,defined_transform)</span><br><span class="line">valid &#x3D; ImageFolder(&#39;C:&#x2F;Users&#x2F;Arthur&#x2F;Desktop&#x2F;sum&#x2F;tempu&#x2F;valid&#x2F;&#39;,defined_transform)</span><br><span class="line">def imshow(input_img):</span><br><span class="line">    input_img &#x3D; input_img.numpy().transpose((1, 2, 0))</span><br><span class="line">    mean &#x3D; np.array([0.485, 0.456, 0.406])</span><br><span class="line">    std &#x3D; np.array([0.229, 0.224, 0.225])</span><br><span class="line">    input_img &#x3D; std * input_img + mean</span><br><span class="line">    input_img &#x3D; np.clip(input_img, 0, 1)</span><br><span class="line">    plt.imshow(input_img)</span><br><span class="line">imshow(train[0][0])</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/xq14183903/xq14183903.github.io@latest/images/dl_practice/3/1.png" alt="" loading="lazy"><em>效果显示</em></p>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>训练数据准备与导入[自定义数据集][分类问题]</title>
      <link href="/2020/07/24/DL_P2/"/>
      <url>/2020/07/24/DL_P2/</url>
      
        <content type="html"><![CDATA[<h4 id="第一步：准备图片"><a href="#第一步：准备图片" class="headerlink" title="第一步：准备图片"></a>第一步：准备图片</h4><p>首先我们需要准备好用于训练和验证的图片，图片的分辨率可以随意，因为pytorch有resize的功能，可以把图片缩放到任意你想要的大小。图片支持类型：.JPG，.PNG…（反正常用的一般都支持）<br>详细参考官网：<br><a href="https://pillow.readthedocs.io/en/5.1.x/handbook/image-file-formats.html" target="_blank" rel="noopener">https://pillow.readthedocs.io/en/5.1.x/handbook/image-file-formats.html</a></p><h4 id="第二步：图片分类"><a href="#第二步：图片分类" class="headerlink" title="第二步：图片分类"></a>第二步：图片分类</h4><p>我们需要创建一个文件夹（最好是和代码文件在同级目录下，虽然我没有这样做哈哈），你可以取名为test啊什么的，反正随意啦。接着我们要在这个文件夹下再创建两个文件夹，分别命名为<strong>train</strong>和<strong>valid</strong>，存放训练数据和验证数据，哈哈别急还没完。我们还需要在这两个文件夹下创建n个文件夹（禁止套娃！），文件夹的个数取决你的分类的类别，有多少类就要创建多少个文件夹，一个文件夹对应一个类。你也可以看下面的导图。创建完后就可以把你准备好的图片按照对应的文件夹进行存放啦</p><p><img src="https://cdn.jsdelivr.net/gh/xq14183903/xq14183903.github.io@latest/images/dl_practice/2/1.jpg" alt="" loading="lazy"><em>文件夹分类导图</em></p><h4 id="第三步：图片导入"><a href="#第三步：图片导入" class="headerlink" title="第三步：图片导入"></a>第三步：图片导入</h4><p>完整代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#常用头文件</span><br><span class="line">from glob import glob</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">from PIL import Image</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import shutil</span><br><span class="line">from torchvision import transforms</span><br><span class="line">from torchvision import models</span><br><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.optim import lr_scheduler</span><br><span class="line">from torch import optim</span><br><span class="line">from torchvision.datasets import ImageFolder</span><br><span class="line">from torchvision.utils import make_grid</span><br><span class="line">import time</span><br><span class="line">import cv2</span><br><span class="line"></span><br><span class="line">train &#x3D; ImageFolder(&#39;C:&#x2F;Users&#x2F;Arthur&#x2F;Desktop&#x2F;sum&#x2F;tempu&#x2F;train&#x2F;&#39;,simple_transform)</span><br><span class="line">valid &#x3D; ImageFolder(&#39;C:&#x2F;Users&#x2F;Arthur&#x2F;Desktop&#x2F;sum&#x2F;tempu&#x2F;valid&#x2F;&#39;,simple_transform)</span><br><span class="line">train_data_gen &#x3D; torch.utils.data.DataLoader(train,shuffle&#x3D;True,batch_size&#x3D;16,num_workers&#x3D;0,drop_last&#x3D;True)</span><br><span class="line">valid_data_gen &#x3D; torch.utils.data.DataLoader(valid,shuffle&#x3D;True,batch_size&#x3D;16,num_workers&#x3D;0,drop_last&#x3D;True)</span><br><span class="line">dataset_sizes &#x3D; &#123;&#39;train&#39;:len(train_data_gen.dataset),&#39;valid&#39;:len(valid_data_gen.dataset)&#125;</span><br><span class="line">dataloaders &#x3D; &#123;&#39;train&#39;:train_data_gen,&#39;valid&#39;:valid_data_gen&#125;</span><br></pre></td></tr></table></figure><p>注：</p><ol><li>如果你的代码文件和你的数据集在同一文件夹下，那么引用的时候就可以不用全路径，如：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train &#x3D; ImageFolder(&#39;&#x2F;tempu&#x2F;train&#x2F;&#39;,simple_transform)</span><br><span class="line">valid &#x3D; ImageFolder(&#39;&#x2F;tempu&#x2F;valid&#x2F;&#39;,simple_transform)</span><br></pre></td></tr></table></figure><ol start="2"><li><strong>simple_transform</strong>是你需要应用的数据变换和增强</li><li><strong>shuffle=True</strong>：训练时随机抽取样本</li><li><strong>drop_last=True</strong>：当你的数据集不能被定义的<strong>batch_size</strong>整除时，多余的部分会被丢掉。</li><li><strong>num_workers=0</strong> 启用多线程个数，一般设置成0是不会有任何问题的。启用多线程可以加快数据集读取速度，<strong>但是</strong>很多时候需要自己调试，有些IDE并不支持。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装Pytorch1.5.1(CUDA)</title>
      <link href="/2020/02/25/DL_P1/"/>
      <url>/2020/02/25/DL_P1/</url>
      
        <content type="html"><![CDATA[<h4 id="前提：需要安装完Anaconda3"><a href="#前提：需要安装完Anaconda3" class="headerlink" title="前提：需要安装完Anaconda3"></a>前提：需要安装完Anaconda3</h4><p>详细请参考此链接：<br><a href="https://www.bilibili.com/read/cv6124348" target="_blank" rel="noopener">https://www.bilibili.com/read/cv6124348</a></p><h4 id="第一步"><a href="#第一步" class="headerlink" title="第一步"></a>第一步</h4><p>打开<strong>开始菜单</strong>，直接输入anaconda，打开 <strong>anaconda prompt</strong><br><img src="/images/dl_practice/1/2.jpg" alt="" loading="lazy"><br>结果：<br><img src="/images/dl_practice/1/3.jpg" alt="" loading="lazy"></p><h4 id="第二步"><a href="#第二步" class="headerlink" title="第二步"></a>第二步</h4><p>我们需要更换清华大学开源下载库(直接从官网下载容易下载失败)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main&#x2F;</span><br><span class="line">conda config --set show_channel_urls yes</span><br><span class="line">conda config --add channels</span><br></pre></td></tr></table></figure><h4 id="第三步"><a href="#第三步" class="headerlink" title="第三步"></a>第三步</h4><p>替换完下载源后我们就可以开始安装啦：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision cudatoolkit&#x3D;10.1</span><br></pre></td></tr></table></figure><p>后面只要跟着提示走就可以了，作者因为已经安装完成所以这里不再做演示<br>若下载中出现网络连接失败情况，请重新尝试几次<br>老版本的pytorch可以进行手动删除或者不删，因为高版本会替换掉低版本</p><h4 id="第四步"><a href="#第四步" class="headerlink" title="第四步"></a>第四步</h4><p>安装完后，我们可以测试一下，可以直接在 anaconda prompt里测试，也可以使用Jupyter, Spyder中进行测试</p><p>Eg.<br><img src="/images/dl_practice/1/1.jpg" alt="" loading="lazy"><br>如果你的结果和我上面这张图一样，那么就<strong>恭喜你！安装成功啦！</strong></p>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
