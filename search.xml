<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>关于深度学习的Python基础知识</title>
      <link href="/2020/08/07/DL_P5/"/>
      <url>/2020/08/07/DL_P5/</url>
      
        <content type="html"><![CDATA[<h4 id="不可变对象和可变对象"><a href="#不可变对象和可变对象" class="headerlink" title="不可变对象和可变对象"></a>不可变对象和可变对象</h4><ul><li>不可变对象：对象对应内存中的值不会变，因此如果指向该对象的变量被改变了，Pyhton则会重新开辟一片内存，变量再指向这个新的内存，包括int、float、str、tuple(元组)等。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 1</span><br><span class="line">b &#x3D; a</span><br><span class="line">b &#x3D; 2</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"># result: a &#x3D; 1</span><br></pre></td></tr></table></figure></li><li>可变对象：对象对应内存中的值可以改变，因此变量改变后，该对象也会改变，即原地修改，如list、dictionary、set等。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; [1]</span><br><span class="line">b &#x3D; a</span><br><span class="line">b.append(2)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"># result: a &#x3D; [1,2]</span><br></pre></td></tr></table></figure></li></ul><p><strong>注：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; [1]</span><br><span class="line">b &#x3D; a</span><br><span class="line">b &#x3D; [1,2]</span><br><span class="line">print(a)</span><br><span class="line"># result: a &#x3D; [1]</span><br></pre></td></tr></table></figure><p>这里的变量a没有发生任何变化因为变量b指向了一个新的内存，详见下列代码进行对比：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; [1]</span><br><span class="line">b &#x3D; a</span><br><span class="line">print(id(a)&#x3D;&#x3D;id(b))</span><br><span class="line">b.append(2)</span><br><span class="line">print(id(a)&#x3D;&#x3D;id(b))</span><br><span class="line"># result: True,True</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; [1]</span><br><span class="line">b &#x3D; a</span><br><span class="line">print(id(a)&#x3D;&#x3D;id(b))</span><br><span class="line">b &#x3D; [1,2]</span><br><span class="line">print(id(a)&#x3D;&#x3D;id(b))</span><br><span class="line"></span><br><span class="line"># result: True,False</span><br></pre></td></tr></table></figure><h4 id="浅拷贝和深拷贝"><a href="#浅拷贝和深拷贝" class="headerlink" title="浅拷贝和深拷贝"></a>浅拷贝和深拷贝</h4><ul><li>浅拷贝：使用copy()函数，拷贝了list最外围，而list内部的对象仍然是引用。</li><li>深拷贝：使用deepcopy()函数，list内外围均为拷贝，因此前后的变量完全隔离，而非引用。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import copy</span><br><span class="line">a &#x3D; [1,2,3,[1,2,3]]</span><br><span class="line">b &#x3D; a                   #变量内外部都是引用</span><br><span class="line">c &#x3D; copy.copy(a)        #浅拷贝，内部是引用</span><br><span class="line">d &#x3D; a[:]                #同上</span><br><span class="line">e &#x3D; copy.deepcopy(a)    #深拷贝，内外部都完全隔离不是引用</span><br><span class="line">a.append(4) </span><br><span class="line">a[3].append(4)</span><br><span class="line">print(b)</span><br><span class="line">print(c)</span><br><span class="line">print(d)</span><br><span class="line">print(e)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line">[1, 2, 3, [1, 2, 3, 4], 4]</span><br><span class="line">[1, 2, 3, [1, 2, 3, 4]]</span><br><span class="line">[1, 2, 3, [1, 2, 3, 4]]</span><br><span class="line">[1, 2, 3, [1, 2, 3]]</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure></li></ul><h4 id="局部变量和全局变量"><a href="#局部变量和全局变量" class="headerlink" title="局部变量和全局变量"></a>局部变量和全局变量</h4><ul><li>局部变量是无法修改全局变量的。想要实现局部修改全局变量，通常有两种办法，增加globa等关键字，或者使用list和dict等可变对象的内置函数。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 1 </span><br><span class="line">def local(): </span><br><span class="line">      a &#x3D; 2     #局部变量，可以看做为新的变量</span><br><span class="line">local()</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"># result: 1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; 1 </span><br><span class="line">b &#x3D; [1]</span><br><span class="line">def local(): </span><br><span class="line">      global a                  #使用global关键字，表明在局部使用的是全局的a变量</span><br><span class="line">      a &#x3D; 2 </span><br><span class="line">      b.append(2)               #对于可变对象，使用内置函数则会修改全局变量</span><br><span class="line">local()</span><br><span class="line">print(a)                        </span><br><span class="line">print(b)                        </span><br><span class="line"></span><br><span class="line"># result: 2,[1,2]</span><br></pre></td></tr></table></figure></li></ul><h4 id="高阶函数"><a href="#高阶函数" class="headerlink" title="高阶函数"></a>高阶函数</h4><ul><li>abs(): 返回数字的绝对值<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(abs(-1))</span><br><span class="line"></span><br><span class="line"># result: 1</span><br></pre></td></tr></table></figure></li><li>map(): 可以将一个函数映射作用到可迭代的序列中，并返回函数输出的序列<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def f(x):</span><br><span class="line">    return x + 1</span><br><span class="line">result &#x3D; list(map(f,[1,2,3])) #python3中输出前需要转化为list</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"># reuslt: [2, 3, 4]</span><br></pre></td></tr></table></figure></li><li>reduce(): 输入的函数需要传入两个参数。reduce()的过程是先使用输入函数对序列中的前两个元素进行操作，得到的结果再和第三个元素进行运算，直到最后一个元素。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from functools import reduce  </span><br><span class="line">def f(x, y):</span><br><span class="line">    return x*10+y</span><br><span class="line">result &#x3D; reduce(f, [1, 2, 3, 4])</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">#result: 1234</span><br></pre></td></tr></table></figure></li><li>filter(): 通过输入函数对可迭代序列进行过滤，并返回满足过滤条件的可迭代序列<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def is_odd(n): </span><br><span class="line">    return n % 2 &#x3D;&#x3D; 0</span><br><span class="line">result &#x3D; list(filter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15]))</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">#result: [2, 4, 6, 10]</span><br></pre></td></tr></table></figure></li><li>sorted(): 函数可以完成对可迭代序列的排序。与列表本身自带的sort()函数不同，这里的sorted()函数返回的是一个新的列表。sorted()函数可以传入关键字key来指定排序的标准，参数reverse代表是否反向。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result &#x3D; list(sorted([3, 5, -87, 0, -21], key&#x3D;abs, reverse&#x3D;True))  # 绝对值排序，并且为反序</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">#result: [-87, -21, 5, 3, 0]</span><br></pre></td></tr></table></figure></li><li>对于一些简单逻辑函数，可以使用lambda匿名表达式来取代函数的定义，类似于def<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">add &#x3D; lambda x,y: x + y</span><br><span class="line">print(add(1,2))</span><br></pre></td></tr></table></figure></li></ul><h4 id="迭代器与生成器"><a href="#迭代器与生成器" class="headerlink" title="迭代器与生成器"></a>迭代器与生成器</h4><ul><li>迭代器(iterator): 不要求事先准备好整个迭代过程中所有的元素，可以使用next()来访问元素。Python中的容器，如list、dict和set等，都属于可迭代对象，对于这些容器，我们可以使用iter()函数封装成迭代器。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; [1,2,3,4,5,6]</span><br><span class="line">y &#x3D; iter(x)</span><br><span class="line">z &#x3D; iter(x)</span><br><span class="line">print(next(y),next(z))</span><br><span class="line"></span><br><span class="line">#result: 1 1</span><br></pre></td></tr></table></figure>上面的例子证明了迭代器之间相互独立</li><li>生成器(generator):是迭代器的一种，可以控制循环遍历的过程，实现一边循环一边计算，并使用yield来返回函数值，每次调用到yield会暂停。生成器迭代的序列可以不是完整的，从而可以节省出大量的内存空间。<br>例：斐波那契数列<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def fibonacci():</span><br><span class="line">    a &#x3D; [1, 1]</span><br><span class="line">    while True:</span><br><span class="line">            a.append(sum(a))        # 往列表里添加下一个元素</span><br><span class="line">            yield a.pop(0)          # 取出第0个元素，并停留在当前执行点</span><br><span class="line">result &#x3D; []</span><br><span class="line">for x in fibonacci():</span><br><span class="line">    if x &gt; 10:    </span><br><span class="line">            break                   # 仅打印小于10的数字</span><br><span class="line">    result.append(x)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">#result: [1, 1, 2, 3, 5, 8]</span><br></pre></td></tr></table></figure></li><li>还可以使用”()”创建生成器<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; (x for x in range(1,10))</span><br><span class="line">print(next(a))</span><br><span class="line"></span><br><span class="line">#result: 1</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于矩阵，向量和标量的转置，相加和相乘</title>
      <link href="/2020/08/05/DL_Theory2/"/>
      <url>/2020/08/05/DL_Theory2/</url>
      
        <content type="html"><![CDATA[<h4 id="转置-transpose"><a href="#转置-transpose" class="headerlink" title="转置(transpose)"></a>转置(transpose)</h4><ul><li>定义：给定m x n矩阵A，则A的转置是一个n x m矩阵，它的列是由A的行组成的</li><li>变量表示：<br>$$ \bm{A}^T $$</li><li>例：给定一个矩阵：<br>$$ \bm{A} = \begin{bmatrix} A_{1,1} &amp; A_{1,2} \cr A_{2,1} &amp; A_{2,2} \end{bmatrix} $$<br>它的转置是：<br>$$ \bm{A}^{T} = \begin{bmatrix} A_{1,1} &amp; A_{2,1} \cr A_{1,2} &amp; A_{2,2} \end{bmatrix} $$</li><li>有时，我们通过将向量元素作为行矩阵写在文本行中，然后使用转置操作将其变为标准的列向量，比如：<br>$$ \bm{x} = [x_1, x_2, x_3]^T $$</li><li>标量的转置等于它本身，即：<br>$$ a = a^T $$</li></ul><h4 id="加法"><a href="#加法" class="headerlink" title="加法"></a>加法</h4><ul><li>定义：只要矩阵的形状一样，我们可以把两个矩阵相加。两个矩阵相加是指对应位置的元素相加，比如：<br>$$ \bm{C} = \bm{A} + \bm{B} $$ 其中 $$ C_{i,j} = A_{i,j} + B_{i,j} $$</li><li>标量和矩阵相加时，我们只需要将其与矩阵的每个元素相加，比如<br>$$ \bm{D} = \bm{B} + c $$ 其中 $$ D_{i,j} = B_{i,j} + c $$</li><li>在深度学习中，我们允许矩阵和向量相加，比如：<br>$$ \bm{C} = \bm{A} + \bm{b} $$ 其中 $$ C_{i,j} = A_{i,j} + b_{j} $$<br>这种隐式地复制向量b到很多位置的方式，称为<strong>广播(broadcasting)</strong></li></ul><h4 id="乘法"><a href="#乘法" class="headerlink" title="乘法"></a>乘法</h4><ul><li>定义：两个矩阵A和B的<strong>矩阵乘积(matrix product)</strong>是第三个矩阵C(n x p)。为了使乘法可被定义，<em>矩阵A(m x n)的列数</em>必须和<em>矩阵B(n x p)的行数</em>相等</li><li>写法：$$ \bm{C} = \bm{A}  \bm{B} $$<br>具体操作方法：<br>$$ C_{i,j} = \displaystyle\sum_{k} A_{i,k} B_{k,j} $$</li></ul><h4 id="参考目录"><a href="#参考目录" class="headerlink" title="参考目录"></a>参考目录</h4><blockquote><p>Goodfellow, I., Bengio, Y. and Courville, A., 2016. Deep learning. MIT press.<br>Lay, D.C., 2016. Linear Algebra and its applications 5th edition. Pearson.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[理论] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>标量、向量、矩阵和张量</title>
      <link href="/2020/08/02/DL_Theory1/"/>
      <url>/2020/08/02/DL_Theory1/</url>
      
        <content type="html"><![CDATA[<h4 id="标量-scalar"><a href="#标量-scalar" class="headerlink" title="标量(scalar)"></a>标量(scalar)</h4><ul><li>定义：一个标量就是一个单独的数</li><li>变量表示：斜体，小写</li><li>例：一个自然数标量<br>$$ n\in\mathbb{N} $$</li></ul><h4 id="向量-vector"><a href="#向量-vector" class="headerlink" title="向量(vector)"></a>向量(vector)</h4><ul><li>定义：一个向量是一列数。这些数是有序排列的。如果向量中的每个元素都属于实数R，并且该向量有n个元素，则记为：<br>$$ \mathbb{R}^n $$</li><li>变量表示：粗体，小写</li><li>例：一个有n个元素的列向量<br>$$ \bm{x} = \begin{bmatrix} x_1 \cr x_2 \cr \vdots \cr x_n \end{bmatrix} $$</li><li>我们用符号“ - ” 表示集合的补集中的索引。例：<br>$$ 指定 x_1, x_3, x_6, 我们定义集合 S = \lbrace 1,3,6  \rbrace,然后写作\bm{x}_S $$</li></ul><p>$$ \bm{x}_{-1}表示\bm{x}中除x_1外的所有元素所构成的向量 $$</p><p>$$ \bm{x}_{-S}表示\bm{x}中除x_1, x_3, x_6外的所有元素所构成的向量 $$</p><h4 id="矩阵-matrix"><a href="#矩阵-matrix" class="headerlink" title="矩阵(matrix)"></a>矩阵(matrix)</h4><ul><li>定义：是一个二维数组</li><li>变量表示：粗体，大写。在表示矩阵中的元素时：斜体，大写</li><li>例：一个2X2矩阵<br>$$ \bm{A} = \begin{bmatrix} A_{1,1} &amp; A_{1,2} \cr A_{2,1} &amp; A_{2,2} \end{bmatrix} $$</li><li>通过用“ ：”表示水平坐标，以表示垂直坐标i中的所有元素。</li><li>例：矩阵的第i行(row)：<br>$$ \bm{A}_{i,:} $$</li><li>例：矩阵的第i列(column)：<br>$$ \bm{A}_{:,i} $$</li><li>表示函数f作用在矩阵上输出的矩阵的第i行第j列元素：<br>$$ f(\bm{A})_{i,j} $$</li></ul><h4 id="张量-tensor"><a href="#张量-tensor" class="headerlink" title="张量(tensor)"></a>张量(tensor)</h4><ul><li>定义：坐标超过两维的数组。一般的，一个数组中的元素分布在若干维坐标的规则网络中，我们称之为张量。</li><li>变量表示：$$ \textsf{\textbf{A}} $$</li></ul><h4 id="参考目录"><a href="#参考目录" class="headerlink" title="参考目录"></a>参考目录</h4><blockquote><p>Goodfellow, I., Bengio, Y. and Courville, A., 2016. Deep learning. MIT press.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[理论] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模型评估测试[分类问题]</title>
      <link href="/2020/07/28/DL_P4/"/>
      <url>/2020/07/28/DL_P4/</url>
      
        <content type="html"><![CDATA[<h4 id="第一步：准备测试图片和测试模型"><a href="#第一步：准备测试图片和测试模型" class="headerlink" title="第一步：准备测试图片和测试模型"></a>第一步：准备测试图片和测试模型</h4><p>准备好一张需要测试的图片和训练好的测试模型，放到测试文件夹中</p><h4 id="第二步：测试代码"><a href="#第二步：测试代码" class="headerlink" title="第二步：测试代码"></a>第二步：测试代码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from PIL import Image</span><br><span class="line">from torchvision import transforms</span><br><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">#加载测试模型</span><br><span class="line">model &#x3D; torch.load(&#39;C:&#x2F;Users&#x2F;Arthur&#x2F;Desktop&#x2F;test&#x2F;model.pth&#39;)</span><br><span class="line">#加载测试图片</span><br><span class="line">test_img &#x3D; Image.open(&#39;C:&#x2F;Users&#x2F;Arthur&#x2F;Desktop&#x2F;test&#x2F;12.png&#39;)</span><br><span class="line">#图片变换</span><br><span class="line">transform &#x3D; transforms.Compose([transforms.Resize((224,224))</span><br><span class="line">                                       ,transforms.ToTensor()</span><br><span class="line">                                       ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])</span><br><span class="line">test_img &#x3D; transform(test_img)</span><br><span class="line">#增加一维度</span><br><span class="line">inputs &#x3D;Variable(torch.unsqueeze(test_img, dim&#x3D;0).float(), requires_grad&#x3D;False).cuda()</span><br><span class="line">outputs &#x3D; model(inputs)</span><br><span class="line">#经过sofamax层后输出概率</span><br><span class="line">probability &#x3D; torch.nn.functional.softmax(outputs,dim&#x3D;1)</span><br><span class="line">#找出最大概率和相应的标签</span><br><span class="line">max_value,index &#x3D; torch.max(probability,1)</span><br><span class="line">print(probability,max_value,index)</span><br></pre></td></tr></table></figure><p><strong>注：</strong></p><ol><li>测试图片的尺寸需要和训练尺寸相同，并且也需要进行张量变换（ToTensor）和归一化（Normalize）。</li><li>由于pytorch要求的输入的维度为[batch_size, channels, width, height]，而一个样本的维度为[channels, width, height]，此时因为我们需要测试一张图片，所以用unsqueeze()增加一个维度变为[1, channels, width, height]。</li><li>index只能给你一个数字，它其实代表的是第几个类别，如果想知道具体类别名称，可以打开训练集的文件夹，那么0就是第一个类别，1就是第二个类别…….以此类推。比如以下是我的训练集。<br><img src="https://cdn.jsdelivr.net/gh/xq14183903/xq14183903.github.io@latest/images/dl_practice/4/1.png" alt="" loading="lazy"><em>训练集文件夹</em><br>那么mn就是第0个类别，mp就是第一个类别。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据集图片变换与增强[transform][augmentation]</title>
      <link href="/2020/07/25/DL_P3/"/>
      <url>/2020/07/25/DL_P3/</url>
      
        <content type="html"><![CDATA[<h3 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h3><p>所有数据集图片的格式必须要求被<strong>PIL</strong>所支持。<br>详细信息请见此篇文章：</p><blockquote><p><a href="https://xq14183903.github.io/2020/07/24/DL_P2/" target="_blank" rel="noopener">https://xq14183903.github.io/2020/07/24/DL_P2/</a></p></blockquote><h3 id="图片变换与增强"><a href="#图片变换与增强" class="headerlink" title="图片变换与增强"></a>图片变换与增强</h3><p>官方文档：</p><blockquote><p><a href="https://pytorch.org/docs/stable/torchvision/transforms.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torchvision/transforms.html</a></p></blockquote><h4 id="CenterCrop（中心裁剪）"><a href="#CenterCrop（中心裁剪）" class="headerlink" title="CenterCrop（中心裁剪）"></a>CenterCrop（中心裁剪）</h4><p><code>transforms.CenterCrop(size)</code><br>size：(高,宽)或(边长)。当输入的是一组高和宽时，图片以中心为原点，设定的高和宽为基础，裁剪图片。当输入的size是一个边长时，裁剪的图片必定为正方形</p><h4 id="ColorJitter（亮度、对比度、饱和度、色调）"><a href="#ColorJitter（亮度、对比度、饱和度、色调）" class="headerlink" title="ColorJitter（亮度、对比度、饱和度、色调）"></a>ColorJitter（亮度、对比度、饱和度、色调）</h4><p><code>transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)</code><br>brightness：（min，max），输入必须为非负数。<br>contrast：同上<br>saturation：同上<br>hue：（min，max），且 -0.5 &lt;= min &lt;= max &lt;= 0.5</p><h4 id="Grayscale（灰度化）"><a href="#Grayscale（灰度化）" class="headerlink" title="Grayscale（灰度化）"></a>Grayscale（灰度化）</h4><p><code>transforms.Grayscale(num_output_channels=1)</code><br>num_output_channels：如果等于1，那么输出的单通道图片。如果等于3，那么输出的是RGB图片</p><h4 id="Pad（填充）"><a href="#Pad（填充）" class="headerlink" title="Pad（填充）"></a>Pad（填充）</h4><p><code>transforms.Pad(padding, fill=0, padding_mode=&#39;constant&#39;)</code><br>padding：（int）填充上下左右四边。（num1，num2）num1对应填充左和右，num2对应填充上和下。（num1，num2，num3，num4），num1、num2、num3、num4分别对应左上右下。<br>fill：(R,G,B) <strong>只能在padding_mode=’constant’时使用</strong>。用于选择填充颜色变换。<br>padding_mode：</p><ul><li>constant：填充为fill设置好的颜色</li><li>edge：根据图片边缘最后一个值进行填充</li><li>reflect：根据图片的反射来填充图片，且不重复图片边缘最后一个值</li><li>symmetric：根据图片的反射来填充图片，且重复图片边缘最后一个值 （对称填充）</li></ul><h4 id="RandomAffine（随机仿射变换）"><a href="#RandomAffine（随机仿射变换）" class="headerlink" title="RandomAffine（随机仿射变换）"></a>RandomAffine（随机仿射变换）</h4><p><code>transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)</code><br> degrees：如果输入的是个int，那么变换角度为-int ~ +int，若输入是个元组（min，max），故变换范围为-min ~ +max<br> translate：（num1，num2），图像进行水平或垂直移动。计算公式为 -img_width * a &lt; dx &lt; img_width * a 和<br> -img_height * b &lt; dy &lt; img_height * b。<br> shear：平行于x轴剪裁，也是有点类似中心放大的感觉。三种种输入可选，（int）（num1，num2）（num1，num2，num3，num4）。前两种的裁剪范围是 -int ~ +int 和 num1 ~ num2，最后一种有点特殊，（num1，num2）是平行于x轴剪裁，（num3，num4）再组合成为另外一个范围，用于平行于y轴剪裁。<br>resample：重采样方式，支持 PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC。<br>fillcolor：和transforms.Pad的一样。</p><h4 id="RandomApply（随机变换应用）"><a href="#RandomApply（随机变换应用）" class="headerlink" title="RandomApply（随机变换应用）"></a>RandomApply（随机变换应用）</h4><p><code>transforms.RandomApply(transforms, p=0.5)</code><br>transforms：使用元组或列表把需要随机应用的变换组合在一起，如 transforms = [transforms.RandomAffine(30, translate=(0,0), scale=(1,2), shear=(1,10,3,4), resample=Image.NEAREST, fillcolor=255)]，可以添加多个。<br>p：应用概率</p><h4 id="RandomChoice（随机变换应用选择）"><a href="#RandomChoice（随机变换应用选择）" class="headerlink" title="RandomChoice（随机变换应用选择）"></a>RandomChoice（随机变换应用选择）</h4><p><code>transforms.RandomChoice(transforms)</code><br>transforms：和RandomApply一样，效果是在设定好的元组或列表里选择一个变换进行应用</p><h4 id="RandomCrop（随机裁剪）"><a href="#RandomCrop（随机裁剪）" class="headerlink" title="RandomCrop（随机裁剪）"></a>RandomCrop（随机裁剪）</h4><p><code>torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode=&#39;constant&#39;)</code><br>size：(高,宽)或(边长)。当输入的是一组高和宽时，图片以中心为原点，设定的高和宽为基础，裁剪图片。当输入的size是一个边长时，裁剪的图片必定为正方形。<br>padding：（int）填充上下左右四边。（num1，num2）num1对应填充左和右，num2对应填充上和下。（num1，num2，num3，num4），num1、num2、num3、num4分别对应左上右下。<br>pad_if_needed：如果裁剪后的图片小于预期值，那么会进行自动填充。<br>fill：(R,G,B) <strong>只能在padding_mode=’constant’时使用</strong>。用于选择填充颜色变换。<br>padding_mode：</p><ul><li>constant：填充为fill设置好的颜色</li><li>edge：根据图片边缘最后一个值进行填充</li><li>reflect：根据图片的反射来填充图片，且不重复图片边缘最后一个值</li><li>symmetric：根据图片的反射来填充图片，且重复图片边缘最后一个值 （对称填充）</li></ul><h4 id="RandomGrayscale（随机灰度化）"><a href="#RandomGrayscale（随机灰度化）" class="headerlink" title="RandomGrayscale（随机灰度化）"></a>RandomGrayscale（随机灰度化）</h4><p><code>transforms.RandomGrayscale(p=0.1)</code><br>p：随机灰度化图片的概率</p><h4 id="RandomGrayscale（随机水平翻折）"><a href="#RandomGrayscale（随机水平翻折）" class="headerlink" title="RandomGrayscale（随机水平翻折）"></a>RandomGrayscale（随机水平翻折）</h4><p><code>transforms.RandomHorizontalFlip(p=0.5)</code><br>p：随机灰度化图片的概率</p><h4 id="RandomOrder（随机变换应用顺序）"><a href="#RandomOrder（随机变换应用顺序）" class="headerlink" title="RandomOrder（随机变换应用顺序）"></a>RandomOrder（随机变换应用顺序）</h4><p><code>transforms.RandomOrder(transforms)</code><br>如果列表里有[1,2,3,4]4种变换，随机进行排序应用。<br>transforms：使用元组或列表把需要随机应用的变换组合在一起，如 transforms = [transforms.RandomAffine(30, translate=(0,0), scale=(1,2), shear=(1,10,3,4), resample=Image.NEAREST, fillcolor=255)]，可以添加多个。</p><h4 id="RandomPerspective（随机透视变换）"><a href="#RandomPerspective（随机透视变换）" class="headerlink" title="RandomPerspective（随机透视变换）"></a>RandomPerspective（随机透视变换）</h4><p><code>transforms.RandomPerspective(distortion_scale=0.5, p=0.5, interpolation=3, fill=0)</code><br>interpolation：Default- Image.BICUBIC（不懂，没用过，直接上官网原版，后续用到了再补）<br>p：随机灰度化图片的概率<br>distortion_scale：形变范围，取值范围是[0,1]，默认为0.5<br>fill：(R,G,B) ，用于选择填充颜色变换。</p><h4 id="RandomResizedCrop（随机调整图像大小裁剪）"><a href="#RandomResizedCrop（随机调整图像大小裁剪）" class="headerlink" title="RandomResizedCrop（随机调整图像大小裁剪）"></a>RandomResizedCrop（随机调整图像大小裁剪）</h4><p><code>transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)</code><br>size：(高,宽)或(边长)。当输入的是一组高和宽时，图片以中心为原点，设定的高和宽为基础，裁剪图片。当输入的size是一个边长时，裁剪的图片必定为正方形。<br>scale：原图尺寸裁剪范围，默认为（0.08，1.0）<br>ratio：原图长宽比裁剪范围，默认为（3/4，4/3）<br>interpolation： Default- PIL.Image.BILINEAR</p><h4 id="RandomRotation（随机旋转）"><a href="#RandomRotation（随机旋转）" class="headerlink" title="RandomRotation（随机旋转）"></a>RandomRotation（随机旋转）</h4><p><code>transforms.RandomRotation(degrees, resample=False, expand=False, center=None, fill=None)</code><br>degrees：如果输入的是个int，那么变换角度为-int ~ +int，若输入是个元组（min，max），故变换范围为-min ~ +max<br>resample：重采样方式，支持 PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC。<br>expand：true，确保旋转后的图片能全部显示在原尺寸。false，旋转后可能丢失些图片边缘信息。<br>center：（x，y）确定旋转中心。默认为图片中心。<br>fill：(R,G,B) ，用于选择填充颜色变换。</p><h4 id="RandomVerticalFlip（随机垂直翻转）"><a href="#RandomVerticalFlip（随机垂直翻转）" class="headerlink" title="RandomVerticalFlip（随机垂直翻转）"></a>RandomVerticalFlip（随机垂直翻转）</h4><p><code>transforms.RandomVerticalFlip(p=0.5)</code><br>p：随机灰度化图片的概率</p><h4 id="Resize（修剪图片尺寸）"><a href="#Resize（修剪图片尺寸）" class="headerlink" title="Resize（修剪图片尺寸）"></a>Resize（修剪图片尺寸）</h4><p><code>transforms.Resize(size, interpolation=2)</code><br>size：(高,宽)或(边长)。当输入的是一组高和宽时，图片以中心为原点，设定的高和宽为基础，裁剪图片。当输入的size是一个边长时，裁剪的图片必定为正方形。<br>interpolation： Default- PIL.Image.BILINEAR</p><h4 id="LinearTransformation（线性变换）"><a href="#LinearTransformation（线性变换）" class="headerlink" title="LinearTransformation（线性变换）"></a>LinearTransformation（线性变换）</h4><p>transforms.LinearTransformation(transformation_matrix, mean_vector)<br>没用过，直接翻译官方应用：白化变换：假设X是一个列向量并中心为0的数据，然后计算该数据的协方差矩阵[D x D]（使用<br> torch.mm(X.t(), X)），计算出来的协方差矩阵再进行奇异值分解然后传入transformation_matrix。<br>mean_vector：平均向量</p><h4 id="Normalize（归一化）"><a href="#Normalize（归一化）" class="headerlink" title="Normalize（归一化）"></a>Normalize（归一化）</h4><p><code>transforms.Normalize(mean, std, inplace=False)</code><br>mean：每个通道的均值<br>std：每个通道的标准差<br>通常为：<code>transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</code></p><h4 id="RandomErasing（随机擦除）"><a href="#RandomErasing（随机擦除）" class="headerlink" title="RandomErasing（随机擦除）"></a>RandomErasing（随机擦除）</h4><p><code>transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)</code><br>p：随机灰度化图片的概率<br>scale：擦除比率范围<br>ratio：擦除长宽高比率范围<br>value：擦除后填补的颜色，同样使用（R，G，B）模式<br>通常放在transforms.Normalize后面</p><h4 id="ToPILImage（PIL图片格式转换）"><a href="#ToPILImage（PIL图片格式转换）" class="headerlink" title="ToPILImage（PIL图片格式转换）"></a>ToPILImage（PIL图片格式转换）</h4><p><code>transforms.ToPILImage(mode=None)</code><br>把一个tensor或narray转换为PIL图片<br>mode：如果输入有4通道，那么建议为RGBA，3通道-RGB，2通道-LA，单通道由数据类型决定，如int、float、short</p><h4 id="ToTensor（张量格式转换）"><a href="#ToTensor（张量格式转换）" class="headerlink" title="ToTensor（张量格式转换）"></a>ToTensor（张量格式转换）</h4><p><code>transforms.ToTensor()</code></p><h4 id="Lambda（应用用户自定义的变换）"><a href="#Lambda（应用用户自定义的变换）" class="headerlink" title="Lambda（应用用户自定义的变换）"></a>Lambda（应用用户自定义的变换）</h4><p><code>transforms.Lambda(lambd)</code></p><h4 id="Compose（存放各种变换应用）"><a href="#Compose（存放各种变换应用）" class="headerlink" title="Compose（存放各种变换应用）"></a>Compose（存放各种变换应用）</h4><p><code>transforms.Compose([])</code></p><h3 id="效果可视化"><a href="#效果可视化" class="headerlink" title="效果可视化"></a>效果可视化</h3><p>完整代码(例)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from PIL import Image</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from torchvision import transforms</span><br><span class="line">from torchvision.datasets import ImageFolder</span><br><span class="line"></span><br><span class="line">defined_transform &#x3D; transforms.Compose([transforms.Resize((32,32)),</span><br><span class="line">                                       transforms.RandomHorizontalFlip(p&#x3D;0.5),</span><br><span class="line">                                       transforms.RandomRotation(30),</span><br><span class="line">                                        transforms.ToTensor(),</span><br><span class="line">                                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])</span><br><span class="line">train &#x3D; ImageFolder(&#39;C:&#x2F;Users&#x2F;Arthur&#x2F;Desktop&#x2F;sum&#x2F;tempu&#x2F;train&#x2F;&#39;,defined_transform)</span><br><span class="line">valid &#x3D; ImageFolder(&#39;C:&#x2F;Users&#x2F;Arthur&#x2F;Desktop&#x2F;sum&#x2F;tempu&#x2F;valid&#x2F;&#39;,defined_transform)</span><br><span class="line">def imshow(input_img):</span><br><span class="line">    input_img &#x3D; input_img.numpy().transpose((1, 2, 0))</span><br><span class="line">    mean &#x3D; np.array([0.485, 0.456, 0.406])</span><br><span class="line">    std &#x3D; np.array([0.229, 0.224, 0.225])</span><br><span class="line">    input_img &#x3D; std * input_img + mean</span><br><span class="line">    input_img &#x3D; np.clip(input_img, 0, 1)</span><br><span class="line">    plt.imshow(input_img)</span><br><span class="line">imshow(train[0][0])</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/xq14183903/xq14183903.github.io@latest/images/dl_practice/3/1.png" alt="" loading="lazy"><em>效果显示</em></p>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>训练数据准备与导入[自定义数据集][分类问题]</title>
      <link href="/2020/07/24/DL_P2/"/>
      <url>/2020/07/24/DL_P2/</url>
      
        <content type="html"><![CDATA[<h4 id="第一步：准备图片"><a href="#第一步：准备图片" class="headerlink" title="第一步：准备图片"></a>第一步：准备图片</h4><p>首先我们需要准备好用于训练和验证的图片，图片的分辨率可以随意，因为pytorch有resize的功能，可以把图片缩放到任意你想要的大小。图片支持类型：.JPG，.PNG…（反正常用的一般都支持）<br>详细参考官网：<br><a href="https://pillow.readthedocs.io/en/5.1.x/handbook/image-file-formats.html" target="_blank" rel="noopener">https://pillow.readthedocs.io/en/5.1.x/handbook/image-file-formats.html</a></p><h4 id="第二步：图片分类"><a href="#第二步：图片分类" class="headerlink" title="第二步：图片分类"></a>第二步：图片分类</h4><p>我们需要创建一个文件夹（最好是和代码文件在同级目录下，虽然我没有这样做哈哈），你可以取名为test啊什么的，反正随意啦。接着我们要在这个文件夹下再创建两个文件夹，分别命名为<strong>train</strong>和<strong>valid</strong>，存放训练数据和验证数据，哈哈别急还没完。我们还需要在这两个文件夹下创建n个文件夹（禁止套娃！），文件夹的个数取决你的分类的类别，有多少类就要创建多少个文件夹，一个文件夹对应一个类。你也可以看下面的导图。创建完后就可以把你准备好的图片按照对应的文件夹进行存放啦</p><p><img src="https://cdn.jsdelivr.net/gh/xq14183903/xq14183903.github.io@latest/images/dl_practice/2/1.jpg" alt="" loading="lazy"><em>文件夹分类导图</em></p><h4 id="第三步：图片导入"><a href="#第三步：图片导入" class="headerlink" title="第三步：图片导入"></a>第三步：图片导入</h4><p>完整代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#常用头文件</span><br><span class="line">from glob import glob</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">from PIL import Image</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import shutil</span><br><span class="line">from torchvision import transforms</span><br><span class="line">from torchvision import models</span><br><span class="line">import torch</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.optim import lr_scheduler</span><br><span class="line">from torch import optim</span><br><span class="line">from torchvision.datasets import ImageFolder</span><br><span class="line">from torchvision.utils import make_grid</span><br><span class="line">import time</span><br><span class="line">import cv2</span><br><span class="line"></span><br><span class="line">train &#x3D; ImageFolder(&#39;C:&#x2F;Users&#x2F;Arthur&#x2F;Desktop&#x2F;sum&#x2F;tempu&#x2F;train&#x2F;&#39;,simple_transform)</span><br><span class="line">valid &#x3D; ImageFolder(&#39;C:&#x2F;Users&#x2F;Arthur&#x2F;Desktop&#x2F;sum&#x2F;tempu&#x2F;valid&#x2F;&#39;,simple_transform)</span><br><span class="line">train_data_gen &#x3D; torch.utils.data.DataLoader(train,shuffle&#x3D;True,batch_size&#x3D;16,num_workers&#x3D;0,drop_last&#x3D;True)</span><br><span class="line">valid_data_gen &#x3D; torch.utils.data.DataLoader(valid,shuffle&#x3D;True,batch_size&#x3D;16,num_workers&#x3D;0,drop_last&#x3D;True)</span><br><span class="line">dataset_sizes &#x3D; &#123;&#39;train&#39;:len(train_data_gen.dataset),&#39;valid&#39;:len(valid_data_gen.dataset)&#125;</span><br><span class="line">dataloaders &#x3D; &#123;&#39;train&#39;:train_data_gen,&#39;valid&#39;:valid_data_gen&#125;</span><br></pre></td></tr></table></figure><p>注：</p><ol><li>如果你的代码文件和你的数据集在同一文件夹下，那么引用的时候就可以不用全路径，如：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train &#x3D; ImageFolder(&#39;&#x2F;tempu&#x2F;train&#x2F;&#39;,simple_transform)</span><br><span class="line">valid &#x3D; ImageFolder(&#39;&#x2F;tempu&#x2F;valid&#x2F;&#39;,simple_transform)</span><br></pre></td></tr></table></figure><ol start="2"><li><strong>simple_transform</strong>是你需要应用的数据变换和增强</li><li><strong>shuffle=True</strong>：训练时随机抽取样本</li><li><strong>drop_last=True</strong>：当你的数据集不能被定义的<strong>batch_size</strong>整除时，多余的部分会被丢掉。</li><li><strong>num_workers=0</strong> 启用多线程个数，一般设置成0是不会有任何问题的。启用多线程可以加快数据集读取速度，<strong>但是</strong>很多时候需要自己调试，有些IDE并不支持。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装Pytorch1.5.1(CUDA)</title>
      <link href="/2020/02/26/DL_P1/"/>
      <url>/2020/02/26/DL_P1/</url>
      
        <content type="html"><![CDATA[<h4 id="前提：需要安装完Anaconda3"><a href="#前提：需要安装完Anaconda3" class="headerlink" title="前提：需要安装完Anaconda3"></a>前提：需要安装完Anaconda3</h4><p>详细请参考此链接：<br><a href="https://www.bilibili.com/read/cv6124348" target="_blank" rel="noopener">https://www.bilibili.com/read/cv6124348</a></p><h4 id="第一步"><a href="#第一步" class="headerlink" title="第一步"></a>第一步</h4><p>打开<strong>开始菜单</strong>，直接输入anaconda，打开 <strong>anaconda prompt</strong><br><img src="/images/dl_practice/1/2.jpg" alt="" loading="lazy"><br>结果：<br><img src="/images/dl_practice/1/3.jpg" alt="" loading="lazy"></p><h4 id="第二步"><a href="#第二步" class="headerlink" title="第二步"></a>第二步</h4><p>我们需要更换清华大学开源下载库(直接从官网下载容易下载失败)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;</span><br><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main&#x2F;</span><br><span class="line">conda config --set show_channel_urls yes</span><br><span class="line">conda config --add channels</span><br></pre></td></tr></table></figure><h4 id="第三步"><a href="#第三步" class="headerlink" title="第三步"></a>第三步</h4><p>替换完下载源后我们就可以开始安装啦：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision cudatoolkit&#x3D;10.1</span><br></pre></td></tr></table></figure><p>后面只要跟着提示走就可以了，作者因为已经安装完成所以这里不再做演示<br>若下载中出现网络连接失败情况，请重新尝试几次<br>老版本的pytorch可以进行手动删除或者不删，因为高版本会替换掉低版本</p><h4 id="第四步"><a href="#第四步" class="headerlink" title="第四步"></a>第四步</h4><p>安装完后，我们可以测试一下，可以直接在 anaconda prompt里测试，也可以使用Jupyter, Spyder中进行测试</p><p>Eg.<br><img src="/images/dl_practice/1/1.jpg" alt="" loading="lazy"><br>如果你的结果和我上面这张图一样，那么就<strong>恭喜你！安装成功啦！</strong></p>]]></content>
      
      
      <categories>
          
          <category> 从零开始的深度学习[Win10][实战] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
