<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#57b5e7"><meta name="author" content="霂水流年"><meta name="copyright" content="霂水流年"><meta name="generator" content="Hexo 4.2.0"><meta name="theme" content="hexo-theme-yun"><title>Tensor[Pytorch][3] | 心记</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="none" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.15/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_stqaphw3j4.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", () => {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
});
</script><link rel="shortcut icon" type="image/svg+xml" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#57b5e7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"root":"/","title":"徐先生的梦境","version":"0.9.3","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"local_search":{"path":"/search.xml"},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><meta name="description" content="ことわる">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensor[Pytorch][3]">
<meta property="og:url" content="http://yoursite.com/2020/08/17/DL_P8/index.html">
<meta property="og:site_name" content="心记">
<meta property="og:description" content="ことわる">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-08-17T08:11:44.433Z">
<meta property="article:modified_time" content="2020-08-17T08:30:09.005Z">
<meta property="article:author" content="霂水流年">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary"></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script defer src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="霂水流年"><img width="96" loading="lazy" src="/xu.jpg" alt="霂水流年"></a><div class="site-author-name"><a href="/about/">霂水流年</a></div><a class="site-name" href="/about/site.html">心记</a><sub class="site-subtitle"></sub><div class="site-desciption">关关雎鸠，在河之洲</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="我的主页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">13</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">4</span></a></div><a class="site-state-item hty-icon-button" href="https://yun.yunyoujun.cn" target="_blank" rel="noopener" title="文档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-settings-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/xq14183903" title="GitHub" target="_blank" style="color:#6e5494"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=94624423" title="网易云音乐" target="_blank" style="color:#C20C0C"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-netease-cloud-music-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.zhihu.com/people/mu-shui-liu-nian-26" title="知乎" target="_blank" style="color:#0084FF"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhihu-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/6023987" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:xq14183903@outlook.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="https://wwwwww537.github.io" target="_blank" rel="noopener" title="我的小伙伴们" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a></div></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#Tensor的自动求导"><span class="toc-number">1.</span> <span class="toc-text">Tensor的自动求导</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#计算图"><span class="toc-number">2.</span> <span class="toc-text">计算图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-Module类"><span class="toc-number">3.</span> <span class="toc-text">nn.Module类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#损失函数的选择"><span class="toc-number">4.</span> <span class="toc-text">损失函数的选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#优化器"><span class="toc-number">5.</span> <span class="toc-text">优化器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reference"><span class="toc-number">6.</span> <span class="toc-text">Reference</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/17/DL_P8/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="霂水流年"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="心记"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Tensor[Pytorch][3]</h1><div class="post-meta"><div class="post-time" style="display:inline-block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <span class="post-meta-icon-text">发表于</span> <time title="创建时间：2020-08-17 09:11:44" itemprop="dateCreated datePublished" datetime="2020-08-17T09:11:44+01:00">2020-08-17</time></div><span class="post-busuanzi"><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span id="busuanzi_value_page_pv"></span></span></span><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category" href="/categories/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Win10-%E5%AE%9E%E6%88%98/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">从零开始的深度学习[Win10][实战]</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#57b5e7;"><h4 id="Tensor的自动求导"><a href="#Tensor的自动求导" class="headerlink" title="Tensor的自动求导"></a>Tensor的自动求导</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a &#x3D; torch.randn(2,3,requires_grad &#x3D; True)</span><br><span class="line">b &#x3D; torch.randn(2,3)</span><br><span class="line">print(b.requires_grad)</span><br><span class="line"># 只要被计算中的任意一个变量需要求导，那么返回的变量也需要求导</span><br><span class="line">c &#x3D; a + b</span><br><span class="line"># 分离变量，返回的变量不需要求导</span><br><span class="line">d &#x3D; a.detach()</span><br><span class="line">b.requires_grad_()</span><br><span class="line"># 显示变量c经过了什么操作</span><br><span class="line">print(c.grad_fn)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">print(b.requires_grad)</span><br><span class="line">print(c.requires_grad)</span><br><span class="line">print(d.requires_grad)</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">False</span><br><span class="line">&lt;AddBackward0 object at 0x000002672E8FDC88&gt;</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure>
<p><strong>grad：</strong>该Tensor对应的梯度，类型为Tensor，并与Tensor同维度。<br><strong>grad_fn：</strong>指向function对象，即该Tensor经过了什么样的操作，用作反向传播的梯度计算，如果该Tensor由用户自己创建，则该grad_fn为None。</p>
<ul>
<li>require_grad参数表示是否需要对该Tensor进行求导，默认为False；设置为True则需要求导，并且依赖于该Tensor的之后的所有节点都需要求导。</li>
<li>默认的Tensor是不需要求导的</li>
</ul>
<h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><p>计算图是PyTorch对于神经网络的具体实现形式，包括每一个数据Tensor及Tensor之间的函数function。<br>Autograd的基本原理是随着每一步Tensor的计算操作，逐渐生成计算图，并将操作的function记录在Tensor的grad_fn中。在前向计算完后，只需对根节点进行backward函数操作，即可从当前根节点自动进行反向传播与梯度计算，从而得到每一个叶子节点的梯度，梯度计算遵循链式求导法则。<br>例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"># 自己生成的，因此都为叶节点</span><br><span class="line">x &#x3D; torch.randn(1)</span><br><span class="line">w &#x3D; torch.ones(1, requires_grad&#x3D;True)</span><br><span class="line">b &#x3D; torch.ones(1, requires_grad&#x3D;True)</span><br><span class="line">print(b)</span><br><span class="line">print(x.is_leaf)</span><br><span class="line">print(w.is_leaf)</span><br><span class="line">print(b.is_leaf)</span><br><span class="line"># 进行前向计算，由计算生成的变量都不是叶节点</span><br><span class="line">y &#x3D; x * w</span><br><span class="line">z &#x3D; y &#x3D; b</span><br><span class="line">print(y.is_leaf)</span><br><span class="line">print(z.is_leaf)</span><br><span class="line">print(w.grad)</span><br><span class="line">print(b.grad)</span><br><span class="line"># 对根节点调用backward()函数，进行梯度反传</span><br><span class="line">z.backward(retain_graph &#x3D; True)</span><br><span class="line">print(w.grad)</span><br><span class="line">print(b.grad)</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">None</span><br><span class="line">None</span><br><span class="line">None</span><br><span class="line">tensor([1.])</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure>
<ul>
<li>backward()函数还有一个需要传入的参数grad_variabels，其代表了根节点的导数，也可以看做根节点各部分的权重系数。因为PyTorch不允许Tensor对Tensor求导，求导时都是标量对于Tensor进行求导，因此，如果根节点是向量，则应配以对应大小的权重，并求和得到标量，再反传。如果根节点的值是标量，则该参数可以省略，默认为1。</li>
<li>当有多个输出需要同时进行梯度反传时，需要将retain_graph设置为True，从而保证在计算多个输出的梯度时互不影响。</li>
</ul>
<h4 id="nn-Module类"><a href="#nn-Module类" class="headerlink" title="nn.Module类"></a>nn.Module类</h4><p>nn.Module是PyTorch提供的神经网络类，并在类中实现了网络各层的定义及前向计算与反向传播机制。在实际使用时，如果想要实现某个神经网络，只需继承nn.Module，在初始化中定义模型结构与参数，在函数forward()中编写网络前向过程即可。<br>例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line"># 首先建立一个全连接的子module，继承nn.Module</span><br><span class="line">class Linear(nn.Module):</span><br><span class="line">   def __init__(self, in_dim, out_dim):</span><br><span class="line">       super(Linear, self).__init__()   # 调用nn.Module的构造函数</span><br><span class="line">       # 使用nn.Parameter来构造需要学习的参数</span><br><span class="line">       self.w &#x3D; nn.Parameter(torch.randn(in_dim, out_dim))</span><br><span class="line">       self.b &#x3D; nn.Parameter(torch.randn(out_dim))</span><br><span class="line">   # 在forward中实现前向传播过程</span><br><span class="line">   def forward(self, x):</span><br><span class="line">        x &#x3D; x.matmul(self.w)            # 使用Tensor.matmul实现矩阵相乘</span><br><span class="line">        y &#x3D; x + self.b.expand_as(x)     # 使用Tensor.expand_as()来保证矩阵形状一致</span><br><span class="line">        return y</span><br><span class="line">class Perception(nn.Module):</span><br><span class="line">   def __init__(self, in_dim, hid_dim, out_dim):</span><br><span class="line">       super(Perception, self).__init__()</span><br><span class="line">       self.layer1 &#x3D; Linear(in_dim, hid_dim)</span><br><span class="line">       self.relu &#x3D; nn.ReLU(inplace&#x3D;True)</span><br><span class="line">       self.layer2 &#x3D; Linear(hid_dim, out_dim)</span><br><span class="line">       self.fc &#x3D; nn.Linear(3, 3)</span><br><span class="line">   def forward(self, x):</span><br><span class="line">       x &#x3D; self.layer1(x)</span><br><span class="line">       y &#x3D; torch.relu(x)                # 使用torch中的relu作为激活函数</span><br><span class="line">       y &#x3D; self.layer2(y)   </span><br><span class="line">       y &#x3D; torch.softmax(y,dim &#x3D; 1)     # 使用torch中的softmax作为激活函数来进行多分类</span><br><span class="line">       return y</span><br><span class="line"># 实例化一个网络，并赋值全连接中的维数，最终输出三维代表了三分类</span><br><span class="line">pcp &#x3D; Perception(3, 3, 3)</span><br><span class="line">print(pcp)</span><br><span class="line"># named_parameters()可以返回学习参数的迭代器，分别为参数名与参数值</span><br><span class="line">for name, parameter in pcp.named_parameters():</span><br><span class="line">    print(name, parameter)</span><br><span class="line">data &#x3D; torch.randn(4,3)</span><br><span class="line">print(pcp(data))</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line">感知器结构：</span><br><span class="line">Perception(</span><br><span class="line">  (layer1): Linear()</span><br><span class="line">  (relu): ReLU(inplace&#x3D;True)</span><br><span class="line">  (layer2): Linear()</span><br><span class="line">)</span><br><span class="line">各个层的参数：</span><br><span class="line">layer1.w Parameter containing:</span><br><span class="line">tensor([[-0.1038, -0.8919,  0.5721],</span><br><span class="line">        [ 1.0064, -0.2040,  1.1663],</span><br><span class="line">        [-1.6619,  0.6891,  0.9040]], requires_grad&#x3D;True)</span><br><span class="line">layer1.b Parameter containing:</span><br><span class="line">tensor([-0.8985, -0.9285,  0.4434], requires_grad&#x3D;True)</span><br><span class="line">layer2.w Parameter containing:</span><br><span class="line">tensor([[-1.2363,  0.2189,  0.2963],</span><br><span class="line">        [-0.1473,  1.8698,  0.7392],</span><br><span class="line">        [-0.7365,  1.0238, -0.2207]], requires_grad&#x3D;True)</span><br><span class="line">layer2.b Parameter containing:</span><br><span class="line">tensor([ 2.0176, -0.2706,  1.5695], requires_grad&#x3D;True)</span><br><span class="line">测试结果：</span><br><span class="line">tensor([[0.5746, 0.0583, 0.3671],</span><br><span class="line">        [0.3566, 0.2445, 0.3988],</span><br><span class="line">        [0.3658, 0.2336, 0.4006],</span><br><span class="line">        [0.4451, 0.1455, 0.4094]], grad_fn&#x3D;&lt;SoftmaxBackward&gt;)</span><br></pre></td></tr></table></figure>
<ul>
<li>nn.Parameter()函数定义了全连接中的ω和b，这是一种特殊的Tensor的构造方法，默认需要求导，即requires_grad为True。</li>
<li>在PyTorch中，还有一个库为nn.functional，同样也提供了很多网络层与函数功能，但与nn.Module不同的是，利用nn.functional定义的网络层不可自动学习参数，还需要使用nn.Parameter封装。nn.functional的设计初衷是对于一些不需要学习参数的层，如激活层、BN（Batch Normalization）层，可以使用nn.functional，这样这些层就不需要在nn.Module中定义了。</li>
<li>当模型中只是简单的前馈网络时，即上一层的输出直接作为下一层的输入，这时可以采用nn.Sequential()模块来快速搭建模型，而不必手动在forward()函数中一层一层地前向传播。因此，如果想快速搭建模型而不考虑中间过程的话，推荐使用nn.Sequential()模块。<br>例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line">class Perception(nn.Module):</span><br><span class="line">   def __init__(self, in_dim, hid_dim, out_dim):</span><br><span class="line">       super(Perception, self).__init__()</span><br><span class="line">       self.layer &#x3D; nn.Sequential(</span><br><span class="line">             nn.Linear(in_dim, hid_dim),</span><br><span class="line">             nn.Sigmoid(),</span><br><span class="line">             nn.Linear(hid_dim, out_dim),</span><br><span class="line">             nn.Sigmoid()</span><br><span class="line">)</span><br><span class="line">   def forward(self, x):</span><br><span class="line">        y &#x3D; self.layer(x)</span><br><span class="line">        return y</span><br><span class="line">        pcp &#x3D; Perception(1000,10000 ,10)</span><br><span class="line">print(pcp)</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">Perception(</span><br><span class="line">  (layer): Sequential(</span><br><span class="line">    (0): Linear(in_features&#x3D;1000, out_features&#x3D;10000, bias&#x3D;True)</span><br><span class="line">    (1): Sigmoid()</span><br><span class="line">    (2): Linear(in_features&#x3D;10000, out_features&#x3D;10, bias&#x3D;True)</span><br><span class="line">    (3): Sigmoid()</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="损失函数的选择"><a href="#损失函数的选择" class="headerlink" title="损失函数的选择"></a>损失函数的选择</h4><p>PyTorch在torch.nn及torch.nn.functional中都提供了各种损失函数，通常来讲，由于损失函数不含有可学习的参数，因此这两者在功能上基本没有区别。</p>
<table>
<thead>
<tr>
<th>问题类型</th>
<th>损失函数</th>
</tr>
</thead>
<tbody><tr>
<td>二分类</td>
<td>nn.CrossEntropyLoss()</td>
</tr>
<tr>
<td>多类别分类</td>
<td>nn.CrossEntropyLoss()</td>
</tr>
<tr>
<td>多标签分类</td>
<td>nn.CrossEntropyLoss()</td>
</tr>
<tr>
<td>回归</td>
<td>MSE</td>
</tr>
<tr>
<td>向量回归</td>
<td>MSE</td>
</tr>
</tbody></table>
<p>例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">from torch import nn</span><br><span class="line">import torch</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"># 首先建立一个全连接的子module，继承nn.Module</span><br><span class="line">class Linear(nn.Module):</span><br><span class="line">   def __init__(self, in_dim, out_dim):</span><br><span class="line">       super(Linear, self).__init__()   # 调用nn.Module的构造函数</span><br><span class="line">       # 使用nn.Parameter来构造需要学习的参数</span><br><span class="line">       self.w &#x3D; nn.Parameter(torch.randn(in_dim, out_dim))</span><br><span class="line">       self.b &#x3D; nn.Parameter(torch.randn(out_dim))</span><br><span class="line">   # 在forward中实现前向传播过程</span><br><span class="line">   def forward(self, x):</span><br><span class="line">        x &#x3D; x.matmul(self.w)            # 使用Tensor.matmul实现矩阵相乘</span><br><span class="line">        y &#x3D; x + self.b.expand_as(x)     # 使用Tensor.expand_as()来保证矩阵形状一致</span><br><span class="line">        return y</span><br><span class="line">class Perception(nn.Module):</span><br><span class="line">   def __init__(self, in_dim, hid_dim, out_dim):</span><br><span class="line">       super(Perception, self).__init__()</span><br><span class="line">       self.layer1 &#x3D; Linear(in_dim, hid_dim)</span><br><span class="line">       self.relu &#x3D; nn.ReLU(inplace&#x3D;True)</span><br><span class="line">       self.layer2 &#x3D; Linear(hid_dim, out_dim)</span><br><span class="line">       self.fc &#x3D; nn.Linear(3, 3)</span><br><span class="line">   def forward(self, x):</span><br><span class="line">       x &#x3D; self.layer1(x)</span><br><span class="line">       y &#x3D; torch.relu(x)                # 使用torch中的relu作为激活函数</span><br><span class="line">       y &#x3D; self.layer2(y)   </span><br><span class="line">       y &#x3D; torch.softmax(y,dim &#x3D; 1)     # 使用torch中的softmax作为激活函数来进行多分类</span><br><span class="line">       return y</span><br><span class="line"># 实例化一个网络，并赋值全连接中的维数，最终输出三维代表了三分类</span><br><span class="line">pcp &#x3D; Perception(3, 3, 3)</span><br><span class="line">print(pcp)</span><br><span class="line"># named_parameters()可以返回学习参数的迭代器，分别为参数名与参数值</span><br><span class="line">for name, parameter in pcp.named_parameters():</span><br><span class="line">    print(name, parameter)</span><br><span class="line">data &#x3D; torch.randn(4,3)</span><br><span class="line">output &#x3D; pcp(data)</span><br><span class="line">print(output)</span><br><span class="line"># 设置标签，每个数为0、1或2三个类别</span><br><span class="line">label &#x3D; torch.Tensor([0, 1, 2, 0]).long()</span><br><span class="line"># 实例化nn中的交叉熵损失类</span><br><span class="line">criterion &#x3D; nn.CrossEntropyLoss()</span><br><span class="line">loss_functional &#x3D; F.cross_entropy(output, label)</span><br><span class="line">print(criterion(output, label), loss_functional)</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">result:</span><br><span class="line"></span><br><span class="line">Perception(</span><br><span class="line">  (layer1): Linear()</span><br><span class="line">  (relu): ReLU(inplace&#x3D;True)</span><br><span class="line">  (layer2): Linear()</span><br><span class="line">  (fc): Linear(in_features&#x3D;3, out_features&#x3D;3, bias&#x3D;True)</span><br><span class="line">)</span><br><span class="line">layer1.w Parameter containing:</span><br><span class="line">tensor([[-0.5057, -0.3412,  1.3228],</span><br><span class="line">        [ 0.2286,  0.7570,  1.0803],</span><br><span class="line">        [-0.1114, -0.8957,  0.1538]], requires_grad&#x3D;True)</span><br><span class="line">layer1.b Parameter containing:</span><br><span class="line">tensor([ 0.6814, -0.4924, -0.3002], requires_grad&#x3D;True)</span><br><span class="line">layer2.w Parameter containing:</span><br><span class="line">tensor([[ 2.3430,  1.0380,  1.0289],</span><br><span class="line">        [ 0.5373,  0.5528, -1.4240],</span><br><span class="line">        [-1.2860,  0.2747,  1.0854]], requires_grad&#x3D;True)</span><br><span class="line">layer2.b Parameter containing:</span><br><span class="line">tensor([-0.5483, -0.8220, -0.1983], requires_grad&#x3D;True)</span><br><span class="line">fc.weight Parameter containing:</span><br><span class="line">tensor([[-0.5237, -0.1501,  0.5000],</span><br><span class="line">        [ 0.2985,  0.4648, -0.1471],</span><br><span class="line">        [-0.4871, -0.3018,  0.2251]], requires_grad&#x3D;True)</span><br><span class="line">fc.bias Parameter containing:</span><br><span class="line">tensor([0.4509, 0.1335, 0.0446], requires_grad&#x3D;True)</span><br><span class="line">tensor([[0.0069, 0.1075, 0.8856],</span><br><span class="line">        [0.0141, 0.1204, 0.8655],</span><br><span class="line">        [0.4050, 0.2080, 0.3870],</span><br><span class="line">        [0.0056, 0.0956, 0.8988]], grad_fn&#x3D;&lt;SoftmaxBackward&gt;)</span><br><span class="line">两者求得的损失值相同</span><br><span class="line">tensor(1.3640, grad_fn&#x3D;&lt;NllLossBackward&gt;) tensor(1.3640, grad_fn&#x3D;&lt;NllLossBackward&gt;)</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure>

<h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><ol>
<li>SGD<br>例：<br><code>optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.002, momentum=0.9,weight_decay=1e-5)</code></li>
<li>Adam<br>例：<br><code>optimizer_ft = optim.Adam(model_ft.parameters(),lr=0.002,weight_decay=1e-4)</code></li>
<li>RMSprop<br>例：<br><code>optimizer_ft = torch.optim.RMSprop(model_ft.parameters(),lr=0.002,alpha=0.9)</code> </li>
</ol>
<h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><blockquote>
<p>深度学习之PyTorch物体检测实战 - 董洪义<br>Subramanian, V., 2018. Deep Learning with PyTorch: A practical approach to building neural network models using PyTorch. Packt Publishing Ltd.</p>
</blockquote>
</div><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="打赏" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span><div id="reward-comment">money money money~ money money~</div><div id="qr" style="display:none;"><div style="display:inline-block"><a href="/alipay.jpg"><img loading="lazy" src="/alipay.jpg" alt="支付宝" title="支付宝"></a><div><span style="color:#00A3EE"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-alipay-line"></use></svg></span></div></div><div style="display:inline-block"><a href="/wechat.png"><img loading="lazy" src="/wechat.png" alt="微信支付" title="微信支付"></a><div><span style="color:#2DC100"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wechat-pay-line"></use></svg></span></div></div></div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>霂水流年</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="http://yoursite.com/2020/08/17/DL_P8/" title="Tensor[Pytorch][3]">http://yoursite.com/2020/08/17/DL_P8/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/08/19/DL_P9/" rel="prev" title="网络模型[Pytorch]"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">网络模型[Pytorch]</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/08/15/DL_P7/" rel="next" title="Tensor[Pytorch][2]"><span class="post-nav-text">Tensor[Pytorch][2]</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div id="comment"><div class="comment-tooltip text-center"><span>点击按钮跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/xq14183903/xq14183903.github.io/issues?q=is:issue+Tensor[Pytorch][3]" target="_blank" rel="noopener">GitHub Issues</a></div><div id="valine-container"></div><script src="https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js"></script><script>function initValine() {
  const valineConfig = {"enable":true,"appId":"lhezWYEOlfT1JUENiOuMhXeA-gzGzoHsz","appKey":"xJsej5fqNjpWVaosFeW83nex","placeholder":"Say anything you want","avatar":null,"pageSize":10,"visitor":false,"highlight":true,"recordIP":true,"enableQQ":true,"el":"#valine-container","lang":"zh-cn"}
  valineConfig.path = window.location.pathname
  new Valine(valineConfig)
}
setTimeout(initValine, 1000)</script></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2020 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> 霂水流年</span></div><div class="powered"><span>由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v4.2.0</span><span class="footer-separator">|</span><span>主题 - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.9.3</span></div><div id="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv" title="总访客量"><span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-user-line"></use></svg></span><span id="busuanzi_value_site_uv"></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv" title="总访问量"><span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg></span><span id="busuanzi_value_site_pv"></span></span></div></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#57b5e7" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="搜索"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search-line"></use></svg></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script src="/js/search/local-search.js" defer></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-close-line"></use></svg></span></div><div class="search-input-container"><input class="search-input" id="local-search-input" type="text" placeholder="搜索..." value=""></div><div id="local-search-result"></div></div></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script></body></html>